{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "from collections import deque\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import deque\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import keras.backend as K\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prioritized Experience Replay\n",
    "\n",
    "### Main Ideas:\n",
    "\n",
    "1. **Enhancing Experience Replay**:\n",
    "   - **Experience Replay** is a fundamental component in deep reinforcement learning that stores past experiences $ (s, a, r, s') $ in a replay buffer.\n",
    "   - **Prioritized Experience Replay (PER)** improves upon standard experience replay by prioritizing experiences that are more informative, allowing the agent to learn more efficiently.\n",
    "\n",
    "2. **Prioritization Based on TD Error**:\n",
    "   - **Temporal-Difference (TD) Error**: PER assigns priorities to experiences based on the magnitude of their TD errors.\n",
    "   - Experiences with higher TD errors are considered more surprising or informative and are sampled more frequently for training.\n",
    "\n",
    "3. **Sampling Mechanism**:\n",
    "   - **Proportional Prioritization**: The probability of sampling an experience is proportional to its priority raised to a power $ \\alpha $.\n",
    "   - **Rank-Based Prioritization**: Alternatively, experiences can be ranked, and sampling probabilities can decrease monotonically with rank.\n",
    "\n",
    "4. **Bias Correction with Importance Sampling**:\n",
    "   - Non-uniform sampling introduces bias in the updates. PER compensates for this by applying **Importance Sampling (IS) Weights** to the updates.\n",
    "   - The IS weights adjust the updates to account for the non-uniform probabilities, ensuring unbiased learning.\n",
    "\n",
    "5. **Adaptive Learning**:\n",
    "   - PER dynamically adjusts the priorities of experiences based on their TD errors, ensuring that the replay buffer remains focused on the most relevant experiences as learning progresses.\n",
    "\n",
    "### Structure of Prioritized Experience Replay:\n",
    "\n",
    "- **Replay Buffer with Priorities**:\n",
    "  - The replay buffer not only stores experiences but also maintains a priority value for each experience.\n",
    "  \n",
    "- **Priority Assignment**:\n",
    "  - Each experience $ i $ is assigned a priority $ p_i $ based on its TD error:\n",
    "    $$\n",
    "    p_i = |\\delta_i| + \\epsilon\n",
    "    $$\n",
    "    where $ \\delta_i = r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) $ is the TD error, and $ \\epsilon $ is a small positive constant to ensure all experiences have a non-zero probability of being sampled.\n",
    "  \n",
    "- **Sampling Process**:\n",
    "  - **Proportional Prioritization**:\n",
    "    $$\n",
    "    P(i) = \\frac{p_i^\\alpha}{\\sum_{k} p_k^\\alpha}\n",
    "    $$\n",
    "    where $ \\alpha $ determines the degree of prioritization ($ \\alpha = 0 $) corresponds to uniform sampling).\n",
    "  \n",
    "  - **Rank-Based Prioritization**:\n",
    "    - Experiences are sorted based on their priority, and sampling probabilities decrease with rank to reduce bias and variance.\n",
    "  \n",
    "- **Importance Sampling Weights**:\n",
    "  - To correct for the bias introduced by prioritized sampling, IS weights $ w_i $ are computed as:\n",
    "    $$\n",
    "    w_i = \\left( \\frac{1}{N} \\cdot \\frac{1}{P(i)} \\right)^\\beta\n",
    "    $$\n",
    "    where $ N $ is the size of the replay buffer, and $ \\beta $ controls the amount of correction ($ \\beta = 0 $ means no correction, and $ \\beta = 1 $ fully compensates for the non-uniform probabilities).\n",
    "  - These weights are typically normalized by the maximum weight in the batch to ensure stability.\n",
    "  \n",
    "- **Updating Priorities**:\n",
    "  - After each training step, the priorities of the sampled experiences are updated based on their new TD errors to reflect their current importance.\n",
    "\n",
    "### Why Prioritized Experience Replay?\n",
    "\n",
    "- **Improved Sample Efficiency**:\n",
    "  - By focusing on more informative experiences, PER enables the agent to learn effectively from fewer samples compared to uniform sampling.\n",
    "  \n",
    "- **Faster Convergence**:\n",
    "  - Prioritizing high-TD-error experiences accelerates the learning process, leading to quicker policy and value function updates.\n",
    "  \n",
    "- **Enhanced Learning Stability**:\n",
    "  - Emphasizing significant transitions reduces the variance introduced by irrelevant or redundant experiences, contributing to more stable learning.\n",
    "  \n",
    "- **Better Performance in Complex Environments**:\n",
    "  - In environments with sparse or delayed rewards, PER ensures that crucial experiences are revisited more frequently, aiding in the discovery of optimal strategies.\n",
    "\n",
    "### Advantages of Prioritized Experience Replay\n",
    "\n",
    "1. **Focused Learning**:\n",
    "   - Directs the learning process towards experiences that have a higher impact on the agent's policy and value estimates.\n",
    "   \n",
    "2. **Reduced Redundancy**:\n",
    "   - Minimizes the repetition of less informative experiences, making more efficient use of the replay buffer.\n",
    "   \n",
    "3. **Adaptive Sampling**:\n",
    "   - Dynamically adjusts sampling probabilities based on the agent's current learning progress, ensuring the replay buffer remains relevant throughout training.\n",
    "\n",
    "### Implementation Details\n",
    "\n",
    "- **Data Structures for Efficient Sampling**:\n",
    "  - **Sum Tree**:\n",
    "    - A binary tree data structure where each parent node is the sum of its child nodes' priorities.\n",
    "    - Enables efficient $ \\mathcal{O}(\\log N) $ time complexity for updating priorities and sampling based on the cumulative distribution.\n",
    "  \n",
    "  - **Heap Structures**:\n",
    "    - Alternative approaches use heap-based structures to maintain sorted priorities, though they may be less efficient than sum trees for certain operations.\n",
    "  \n",
    "- **Hyperparameters**:\n",
    "  - **$\\alpha$**: Controls the degree of prioritization. Common values range between 0.4 and 0.6.\n",
    "  - **$\\beta$**: Adjusts the amount of importance-sampling correction. It is often annealed from a lower value to 1 over the course of training.\n",
    "  - **$\\epsilon$**: A small constant (e.g., $1 \\times 10^{-6}$) to ensure all experiences have a non-zero probability of being sampled.\n",
    "  \n",
    "- **Integration with Deep Q-Networks (DQN)**:\n",
    "  - PER is typically integrated into the DQN framework by modifying the experience replay buffer to handle priorities and implementing the sampling and weighting mechanisms during training.\n",
    "  \n",
    "- **Handling Edge Cases**:\n",
    "  - Ensuring that the replay buffer does not become overly skewed by a few high-priority experiences.\n",
    "  - Balancing exploration and exploitation by appropriately setting $ \\alpha $ and $ \\beta $.\n",
    "\n",
    "### Advantages Over Uniform Experience Replay\n",
    "\n",
    "- **Focused Learning**: Directs the learning process towards more significant experiences, enhancing the agent's ability to learn optimal policies.\n",
    "- **Reduced Redundancy**: Minimizes the repetition of less informative experiences, making more efficient use of the replay buffer.\n",
    "- **Adaptive Sampling**: Dynamically adjusts the sampling probabilities based on the agent's learning progress, ensuring that the replay buffer remains relevant throughout training.\n",
    "\n",
    "### Potential Drawbacks and Considerations\n",
    "\n",
    "1. **Increased Computational Overhead**:\n",
    "   - Managing priorities and maintaining data structures like sum trees can introduce additional computational complexity compared to uniform sampling.\n",
    "   \n",
    "2. **Bias Introduction**:\n",
    "   - Non-uniform sampling can bias the learning process. Although importance-sampling weights mitigate this bias, careful tuning of hyperparameters is necessary.\n",
    "   \n",
    "3. **Hyperparameter Sensitivity**:\n",
    "   - The performance of PER is sensitive to the choice of hyperparameters ($ \\alpha $, $ \\beta $, $ \\epsilon $), requiring thorough experimentation and tuning.\n",
    "   \n",
    "4. **Implementation Complexity**:\n",
    "   - Implementing efficient PER requires a more sophisticated replay buffer compared to standard uniform experience replay, increasing the complexity of the RL pipeline.\n",
    "\n",
    "### Variants and Extensions\n",
    "\n",
    "1. **Stochastic Prioritized Experience Replay**:\n",
    "   - Introduces randomness in the prioritization process to balance exploration and exploitation.\n",
    "   \n",
    "2. **Max-Prioritized Experience Replay**:\n",
    "   - Focuses exclusively on experiences with the maximum priority, though it can lead to overfitting if not carefully managed.\n",
    "   \n",
    "3. **Multi-Step Prioritized Experience Replay**:\n",
    "   - Extends PER to multi-step returns, allowing the agent to learn from longer sequences of experiences with prioritized sampling.\n",
    "\n",
    "### Integration with Other RL Techniques\n",
    "\n",
    "1. **Dueling Networks**:\n",
    "   - Combining PER with dueling DQN architectures can further enhance learning by focusing on both state-value and action advantages.\n",
    "   \n",
    "2. **Double DQN**:\n",
    "   - When used alongside Double DQN, PER helps in mitigating overestimation bias while efficiently sampling important experiences.\n",
    "   \n",
    "3. **Rainbow DQN**:\n",
    "   - PER is one of the key components in the Rainbow DQN framework, which integrates multiple improvements to create a more robust and high-performing RL agent.\n",
    "\n",
    "### Empirical Results\n",
    "\n",
    "- **Benchmark Performance**:\n",
    "  - Studies have demonstrated that PER significantly improves the performance of DQN agents on various benchmark tasks, including Atari games and continuous control environments.\n",
    "  \n",
    "- **Sample Efficiency**:\n",
    "  - PER agents often achieve higher rewards with fewer training steps compared to agents using uniform experience replay.\n",
    "  \n",
    "- **Stability and Convergence**:\n",
    "  - PER contributes to more stable learning curves and faster convergence rates, particularly in environments with high-dimensional state spaces or sparse rewards.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "**Prioritized Experience Replay** is a powerful enhancement to the standard experience replay mechanism in reinforcement learning. By intelligently prioritizing more informative experiences, PER boosts sample efficiency, accelerates learning, and improves overall agent performance. While it introduces additional complexity and requires careful tuning, the benefits it offers make it a valuable component in modern deep reinforcement learning architectures. When combined with other advancements like dueling networks and Double DQN, PER plays a crucial role in pushing the boundaries of what RL agents can achieve in complex environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PER():\n",
    "    def __init__(self, \n",
    "                 observation_space, \n",
    "                 action_space, \n",
    "                 gamma=0.99, \n",
    "                 lr=0.001,\n",
    "                 buffer_size=20000,\n",
    "                 batch_size=32,\n",
    "                 epsilon_decay=0.99,\n",
    "                 epsilon= 0.7):\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon = epsilon\n",
    "        self.buffer = deque(maxlen=self.buffer_size)\n",
    "        self.model = self.build_model(name='model')\n",
    "        self.target = self.build_model(name='target')\n",
    "        \n",
    "    def build_model(self, name):\n",
    "        model = keras.Sequential(name=name)\n",
    "        model.add(keras.Input(shape=self.observation_space))\n",
    "        model.add(keras.layers.Dense(128, activation='relu'))\n",
    "        model.add(keras.layers.Dense(128, activation='relu'))\n",
    "        model.add(keras.layers.Dense(128, activation='relu'))\n",
    "        model.add(keras.layers.Dense(self.action_space, activation='linear'))\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.legacy.Adam(learning_rate=self.lr),\n",
    "            loss='mse'\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    def predict(self, observation):\n",
    "        return self.model.predict(np.array([observation]), verbose=False)[0]\n",
    "    \n",
    "    def predict_action(self, observation):\n",
    "        return np.argmax(self.predict(observation))\n",
    "    \n",
    "    def e_greedy(self, observation):\n",
    "        if len(self.buffer)%5==0 and self.epsilon > 0.01:\n",
    "            self.epsilon = self.epsilon*self.epsilon_decay\n",
    "        e = self.epsilon\n",
    "        if random.random() >= e:\n",
    "            return self.predict_action(observation)\n",
    "        return random.randint(0, self.action_space-1)\n",
    "    \n",
    "    def remember(self, experience):\n",
    "        initial_priority = max(self.priorities, default=1)\n",
    "        self.buffer.append((experience, initial_priority))\n",
    "    \n",
    "    def get_probabilities(self, priority_scale):\n",
    "        priorities = np.array([priority for _,priority in self.buffer])\n",
    "        scaled_priorities = priorities ** priority_scale\n",
    "        sample_probabilities = scaled_priorities / sum(scaled_priorities)\n",
    "        return sample_probabilities\n",
    "    \n",
    "    def get_importance(self, probabilities, beta = 0.8):\n",
    "        importance = ((1 / len(self.buffer))*(1 / probabilities))**beta\n",
    "        importance_normalized = importance / max(importance)\n",
    "        return importance_normalized\n",
    "    \n",
    "\n",
    "    def sample(self, batch_size = None, priority_scale = 0.1):\n",
    "        if batch_size is None:\n",
    "            batch_size = self.batch_size\n",
    "\n",
    "        sample_probs = self.get_probabilities(priority_scale)\n",
    "        sample_indices = random.choices(range(len(self.buffer)), k=batch_size, weights=sample_probs)\n",
    "        \n",
    "        # Extract experiences separately to avoid shape issues\n",
    "        experiences = [self.buffer[i][0] for i in sample_indices]  # Assuming experience is the first element in tuple\n",
    "        importance = self.get_importance(sample_probs[sample_indices])\n",
    "        \n",
    "        # Structure data by type for training\n",
    "        states = np.array([exp[0] for exp in experiences])\n",
    "        actions = np.array([exp[1] for exp in experiences])\n",
    "        rewards = np.array([exp[2] for exp in experiences])\n",
    "        next_states = np.array([exp[3] for exp in experiences])\n",
    "        dones = np.array([exp[4] for exp in experiences])\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones), importance, sample_indices\n",
    "\n",
    "    \n",
    "    def set_priorities(self, indices, errors, offset=0.1):\n",
    "        for idx, error in zip(indices, errors):\n",
    "            _,priority = self.buffer[idx]\n",
    "            self.buffer[idx] = (self.buffer[idx][0], abs(error)+offset)\n",
    "\n",
    "    def target_update(self):\n",
    "        self.target.set_weights(self.model.get_weights())\n",
    "    \n",
    "    def train(self):\n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        (states, actions, rewards, next_states, dones), importance_weights, indices = self.sample(self.batch_size)\n",
    "\n",
    "        current_q_values = self.model.predict(np.array(states))\n",
    "\n",
    "        next_q_values = self.target.predict(np.array(next_states))\n",
    "\n",
    "        targets = np.array(current_q_values)\n",
    "\n",
    "        max_next_q_values = np.max(next_q_values, axis=1)\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            if dones[i]:\n",
    "                targets[i, actions[i]] = rewards[i]\n",
    "            else:\n",
    "                targets[i, actions[i]] = rewards[i] + self.gamma * max_next_q_values[i]\n",
    "\n",
    "        td_errors = targets[np.arange(self.batch_size), actions] - current_q_values[np.arange(self.batch_size), actions]\n",
    "\n",
    "        history = self.model.fit(np.array(states), targets, sample_weight=importance_weights, verbose=0)\n",
    "\n",
    "        self.set_priorities(indices, td_errors)\n",
    "\n",
    "        self.target_update()\n",
    "\n",
    "        # Extract loss from history object and return it along with other metrics if necessary\n",
    "        loss = history.history['loss'][0]  \n",
    "        return loss, targets, current_q_values\n",
    "\n",
    "    @property\n",
    "    def priorities(self):\n",
    "        return [priority for _, priority in self.buffer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\anaconda3\\envs\\test\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "c:\\Users\\admin\\anaconda3\\envs\\test\\lib\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 - Total Reward: -219.7254431623344, Moving Average Reward: -219.73, Epsilon: 0.9950\n",
      "Episode 2 - Total Reward: -75.5704220783338, Moving Average Reward: -147.65, Epsilon: 0.9900\n",
      "Episode 3 - Total Reward: -84.63763906175664, Moving Average Reward: -126.64, Epsilon: 0.9851\n",
      "Episode 4 - Total Reward: -81.32546350512915, Moving Average Reward: -115.31, Epsilon: 0.9801\n",
      "Episode 5 - Total Reward: -282.9260970304337, Moving Average Reward: -148.84, Epsilon: 0.9752\n",
      "Episode 6 - Total Reward: -66.88187757918828, Moving Average Reward: -135.18, Epsilon: 0.9704\n",
      "Episode 7 - Total Reward: -110.25073635901069, Moving Average Reward: -131.62, Epsilon: 0.9655\n",
      "Episode 8 - Total Reward: -201.99259012271247, Moving Average Reward: -140.41, Epsilon: 0.9607\n",
      "Episode 9 - Total Reward: -140.997095912345, Moving Average Reward: -140.48, Epsilon: 0.9559\n",
      "Episode 10 - Total Reward: -57.77904856929878, Moving Average Reward: -132.21, Epsilon: 0.9511\n",
      "Episode 11 - Total Reward: -243.29453039355727, Moving Average Reward: -142.31, Epsilon: 0.9464\n",
      "Episode 12 - Total Reward: -170.2072277165057, Moving Average Reward: -144.63, Epsilon: 0.9416\n",
      "Episode 13 - Total Reward: -321.07291212416226, Moving Average Reward: -158.20, Epsilon: 0.9369\n",
      "Episode 14 - Total Reward: -115.0511199565396, Moving Average Reward: -155.12, Epsilon: 0.9322\n",
      "Episode 15 - Total Reward: -62.17913878486232, Moving Average Reward: -148.93, Epsilon: 0.9276\n",
      "Episode 16 - Total Reward: -315.11510585262766, Moving Average Reward: -159.31, Epsilon: 0.9229\n",
      "Episode 17 - Total Reward: -275.7143989453888, Moving Average Reward: -166.16, Epsilon: 0.9183\n",
      "Episode 18 - Total Reward: -263.7325927553379, Moving Average Reward: -171.58, Epsilon: 0.9137\n",
      "Episode 19 - Total Reward: -113.99986190325536, Moving Average Reward: -168.55, Epsilon: 0.9092\n",
      "Episode 20 - Total Reward: -89.34628316131293, Moving Average Reward: -164.59, Epsilon: 0.9046\n",
      "Episode 21 - Total Reward: -113.79991028536335, Moving Average Reward: -162.17, Epsilon: 0.9001\n",
      "Episode 22 - Total Reward: -169.8867719453546, Moving Average Reward: -162.52, Epsilon: 0.8956\n",
      "Episode 23 - Total Reward: 31.68989611843429, Moving Average Reward: -154.08, Epsilon: 0.8911\n",
      "Episode 24 - Total Reward: -118.83430312250648, Moving Average Reward: -152.61, Epsilon: 0.8867\n",
      "Episode 25 - Total Reward: -294.8780618994596, Moving Average Reward: -158.30, Epsilon: 0.8822\n",
      "Episode 26 - Total Reward: -115.26978522535376, Moving Average Reward: -156.65, Epsilon: 0.8778\n",
      "Episode 27 - Total Reward: -104.13511489279357, Moving Average Reward: -154.70, Epsilon: 0.8734\n",
      "Episode 28 - Total Reward: -224.4108934238605, Moving Average Reward: -157.19, Epsilon: 0.8691\n",
      "Episode 29 - Total Reward: -106.72154217487424, Moving Average Reward: -155.45, Epsilon: 0.8647\n",
      "Episode 30 - Total Reward: -330.5615056083193, Moving Average Reward: -161.29, Epsilon: 0.8604\n",
      "Episode 31 - Total Reward: -105.36187162512684, Moving Average Reward: -159.48, Epsilon: 0.8561\n",
      "Episode 32 - Total Reward: -92.70277212760756, Moving Average Reward: -157.40, Epsilon: 0.8518\n",
      "Episode 33 - Total Reward: -89.84633203417228, Moving Average Reward: -155.35, Epsilon: 0.8475\n",
      "Episode 34 - Total Reward: -16.98966961480197, Moving Average Reward: -151.28, Epsilon: 0.8433\n",
      "Episode 35 - Total Reward: -95.74685651285665, Moving Average Reward: -149.69, Epsilon: 0.8391\n",
      "Episode 36 - Total Reward: -142.69213774698335, Moving Average Reward: -149.50, Epsilon: 0.8349\n",
      "Episode 37 - Total Reward: -113.68987402730346, Moving Average Reward: -148.53, Epsilon: 0.8307\n",
      "Episode 38 - Total Reward: -172.4084380184289, Moving Average Reward: -149.16, Epsilon: 0.8266\n",
      "Episode 39 - Total Reward: -110.40566744645234, Moving Average Reward: -148.17, Epsilon: 0.8224\n",
      "Episode 40 - Total Reward: -274.8509804441088, Moving Average Reward: -151.33, Epsilon: 0.8183\n",
      "Episode 41 - Total Reward: -259.04177967942144, Moving Average Reward: -153.96, Epsilon: 0.8142\n",
      "Episode 42 - Total Reward: -229.3952939460268, Moving Average Reward: -155.76, Epsilon: 0.8102\n",
      "Episode 43 - Total Reward: -117.69202268456567, Moving Average Reward: -154.87, Epsilon: 0.8061\n",
      "Episode 44 - Total Reward: -125.40977040062452, Moving Average Reward: -154.20, Epsilon: 0.8021\n",
      "Episode 45 - Total Reward: -298.95253109979615, Moving Average Reward: -157.42, Epsilon: 0.7981\n",
      "Episode 46 - Total Reward: -4.624582245826048, Moving Average Reward: -154.10, Epsilon: 0.7941\n",
      "Episode 47 - Total Reward: -118.082434763885, Moving Average Reward: -153.33, Epsilon: 0.7901\n",
      "Episode 48 - Total Reward: -126.21645647319757, Moving Average Reward: -152.76, Epsilon: 0.7862\n",
      "Episode 49 - Total Reward: -156.673540356029, Moving Average Reward: -152.84, Epsilon: 0.7822\n",
      "Episode 50 - Total Reward: -84.96307864351428, Moving Average Reward: -151.49, Epsilon: 0.7783\n",
      "Episode 51 - Total Reward: -93.40803696405843, Moving Average Reward: -150.35, Epsilon: 0.7744\n",
      "Episode 52 - Total Reward: -88.730571807536, Moving Average Reward: -149.16, Epsilon: 0.7705\n",
      "Episode 53 - Total Reward: -90.42374993696794, Moving Average Reward: -148.06, Epsilon: 0.7667\n",
      "Episode 54 - Total Reward: -62.25649052797818, Moving Average Reward: -146.47, Epsilon: 0.7629\n",
      "Episode 55 - Total Reward: -84.60161595875886, Moving Average Reward: -145.34, Epsilon: 0.7590\n",
      "Episode 56 - Total Reward: -244.52755789214285, Moving Average Reward: -147.11, Epsilon: 0.7553\n",
      "Episode 57 - Total Reward: -107.75673350744889, Moving Average Reward: -146.42, Epsilon: 0.7515\n",
      "Episode 58 - Total Reward: -124.57319460542725, Moving Average Reward: -146.05, Epsilon: 0.7477\n",
      "Episode 59 - Total Reward: -160.83988560668598, Moving Average Reward: -146.30, Epsilon: 0.7440\n",
      "Episode 60 - Total Reward: -103.87544084170626, Moving Average Reward: -145.59, Epsilon: 0.7403\n",
      "Episode 61 - Total Reward: -171.74011671339366, Moving Average Reward: -146.02, Epsilon: 0.7366\n",
      "Episode 62 - Total Reward: -113.7694431475991, Moving Average Reward: -145.50, Epsilon: 0.7329\n",
      "Episode 63 - Total Reward: -203.5874223314415, Moving Average Reward: -146.42, Epsilon: 0.7292\n",
      "Episode 64 - Total Reward: -64.29933371650029, Moving Average Reward: -145.14, Epsilon: 0.7256\n",
      "Episode 65 - Total Reward: -60.28734089728809, Moving Average Reward: -143.83, Epsilon: 0.7219\n",
      "Episode 66 - Total Reward: -87.64638926653349, Moving Average Reward: -142.98, Epsilon: 0.7183\n",
      "Episode 67 - Total Reward: -107.10671453184143, Moving Average Reward: -142.44, Epsilon: 0.7147\n",
      "Episode 68 - Total Reward: -75.47855703077595, Moving Average Reward: -141.46, Epsilon: 0.7112\n",
      "Episode 69 - Total Reward: -103.02800089476789, Moving Average Reward: -140.90, Epsilon: 0.7076\n",
      "Episode 70 - Total Reward: -34.328190207214675, Moving Average Reward: -139.38, Epsilon: 0.7041\n",
      "Episode 71 - Total Reward: -82.63797274624505, Moving Average Reward: -138.58, Epsilon: 0.7005\n",
      "Episode 72 - Total Reward: -71.06276153248517, Moving Average Reward: -137.64, Epsilon: 0.6970\n",
      "Episode 73 - Total Reward: -84.74980579674416, Moving Average Reward: -136.92, Epsilon: 0.6936\n",
      "Episode 74 - Total Reward: -116.08606405454478, Moving Average Reward: -136.64, Epsilon: 0.6901\n",
      "Episode 75 - Total Reward: -212.03901322176125, Moving Average Reward: -137.64, Epsilon: 0.6866\n",
      "Episode 76 - Total Reward: -232.74138293985493, Moving Average Reward: -138.89, Epsilon: 0.6832\n",
      "Episode 77 - Total Reward: -48.678074260106314, Moving Average Reward: -137.72, Epsilon: 0.6798\n",
      "Episode 78 - Total Reward: -93.03562471165971, Moving Average Reward: -137.15, Epsilon: 0.6764\n",
      "Episode 79 - Total Reward: -39.949059140050096, Moving Average Reward: -135.92, Epsilon: 0.6730\n",
      "Episode 80 - Total Reward: -231.22422748065824, Moving Average Reward: -137.11, Epsilon: 0.6696\n",
      "Episode 81 - Total Reward: -136.64324542395974, Moving Average Reward: -137.10, Epsilon: 0.6663\n",
      "Episode 82 - Total Reward: -67.54352937005828, Moving Average Reward: -136.26, Epsilon: 0.6630\n",
      "Episode 83 - Total Reward: -38.70690626368744, Moving Average Reward: -135.08, Epsilon: 0.6597\n",
      "Episode 84 - Total Reward: -120.21061856500977, Moving Average Reward: -134.90, Epsilon: 0.6564\n",
      "Episode 85 - Total Reward: -31.72821908916515, Moving Average Reward: -133.69, Epsilon: 0.6531\n",
      "Episode 86 - Total Reward: -85.9944877871812, Moving Average Reward: -133.14, Epsilon: 0.6498\n",
      "Episode 87 - Total Reward: -295.30787934010004, Moving Average Reward: -135.00, Epsilon: 0.6466\n",
      "Episode 88 - Total Reward: -75.06892329867556, Moving Average Reward: -134.32, Epsilon: 0.6433\n",
      "Episode 89 - Total Reward: -120.05765603887706, Moving Average Reward: -134.16, Epsilon: 0.6401\n",
      "Episode 90 - Total Reward: -83.12925329299136, Moving Average Reward: -133.59, Epsilon: 0.6369\n",
      "Episode 91 - Total Reward: -112.12812458688913, Moving Average Reward: -133.36, Epsilon: 0.6337\n",
      "Episode 92 - Total Reward: -164.59778620338392, Moving Average Reward: -133.69, Epsilon: 0.6306\n",
      "Episode 93 - Total Reward: -41.427292000257296, Moving Average Reward: -132.70, Epsilon: 0.6274\n",
      "Episode 94 - Total Reward: -77.12542395452577, Moving Average Reward: -132.11, Epsilon: 0.6243\n",
      "Episode 95 - Total Reward: -41.411807826738595, Moving Average Reward: -131.16, Epsilon: 0.6211\n",
      "Episode 96 - Total Reward: 5.7160842840112025, Moving Average Reward: -129.73, Epsilon: 0.6180\n",
      "Episode 97 - Total Reward: -53.8797415829569, Moving Average Reward: -128.95, Epsilon: 0.6149\n",
      "Episode 98 - Total Reward: -79.8009150258756, Moving Average Reward: -128.45, Epsilon: 0.6119\n",
      "Episode 99 - Total Reward: -69.35039023251065, Moving Average Reward: -127.85, Epsilon: 0.6088\n",
      "Episode 100 - Total Reward: -92.67818450892308, Moving Average Reward: -127.50, Epsilon: 0.6058\n",
      "Episode 101 - Total Reward: -119.72510372481523, Moving Average Reward: -126.50, Epsilon: 0.6027\n",
      "Episode 102 - Total Reward: -103.90520459008098, Moving Average Reward: -126.78, Epsilon: 0.5997\n",
      "Episode 103 - Total Reward: -72.38961610034634, Moving Average Reward: -126.66, Epsilon: 0.5967\n",
      "Episode 104 - Total Reward: -45.11669561973092, Moving Average Reward: -126.30, Epsilon: 0.5937\n",
      "Episode 105 - Total Reward: -68.1517435207497, Moving Average Reward: -124.15, Epsilon: 0.5908\n",
      "Episode 106 - Total Reward: -144.02937900344835, Moving Average Reward: -124.92, Epsilon: 0.5878\n",
      "Episode 107 - Total Reward: -5.603882535881141, Moving Average Reward: -123.88, Epsilon: 0.5849\n",
      "Episode 108 - Total Reward: -48.31803264141473, Moving Average Reward: -122.34, Epsilon: 0.5820\n",
      "Episode 109 - Total Reward: -17.87919819657489, Moving Average Reward: -121.11, Epsilon: 0.5790\n",
      "Episode 110 - Total Reward: -59.632144651047355, Moving Average Reward: -121.13, Epsilon: 0.5762\n",
      "Episode 111 - Total Reward: -144.59923060491465, Moving Average Reward: -120.14, Epsilon: 0.5733\n",
      "Episode 112 - Total Reward: -53.27266737824269, Moving Average Reward: -118.97, Epsilon: 0.5704\n",
      "Episode 113 - Total Reward: -144.9493472860953, Moving Average Reward: -117.21, Epsilon: 0.5676\n",
      "Episode 114 - Total Reward: -44.11582966421604, Moving Average Reward: -116.50, Epsilon: 0.5647\n",
      "Episode 115 - Total Reward: -46.07735078309062, Moving Average Reward: -116.34, Epsilon: 0.5619\n",
      "Episode 116 - Total Reward: 11.840683640744047, Moving Average Reward: -113.07, Epsilon: 0.5591\n",
      "Episode 117 - Total Reward: -53.615126134326985, Moving Average Reward: -110.85, Epsilon: 0.5563\n",
      "Episode 118 - Total Reward: -15.722603714038954, Moving Average Reward: -108.37, Epsilon: 0.5535\n",
      "Episode 119 - Total Reward: -35.645360657257626, Moving Average Reward: -107.58, Epsilon: 0.5507\n",
      "Episode 120 - Total Reward: -96.03016749245165, Moving Average Reward: -107.65, Epsilon: 0.5480\n",
      "Episode 121 - Total Reward: -24.410272740935383, Moving Average Reward: -106.76, Epsilon: 0.5452\n",
      "Episode 122 - Total Reward: -88.10618664695781, Moving Average Reward: -105.94, Epsilon: 0.5425\n",
      "Episode 123 - Total Reward: -73.74670312687122, Moving Average Reward: -106.99, Epsilon: 0.5398\n",
      "Episode 124 - Total Reward: -148.44653880445543, Moving Average Reward: -107.29, Epsilon: 0.5371\n",
      "Episode 125 - Total Reward: -79.34158757196833, Moving Average Reward: -105.13, Epsilon: 0.5344\n",
      "Episode 126 - Total Reward: -253.14852781546995, Moving Average Reward: -106.51, Epsilon: 0.5318\n",
      "Episode 127 - Total Reward: -224.9711460009899, Moving Average Reward: -107.72, Epsilon: 0.5291\n",
      "Episode 128 - Total Reward: -52.05008481550766, Moving Average Reward: -106.00, Epsilon: 0.5264\n",
      "Episode 129 - Total Reward: -53.535818146870156, Moving Average Reward: -105.47, Epsilon: 0.5238\n",
      "Episode 130 - Total Reward: -89.55496822316476, Moving Average Reward: -103.06, Epsilon: 0.5212\n",
      "Episode 131 - Total Reward: -31.665894207860873, Moving Average Reward: -102.32, Epsilon: 0.5186\n",
      "Episode 132 - Total Reward: 4.063776703139453, Moving Average Reward: -101.35, Epsilon: 0.5160\n",
      "Episode 133 - Total Reward: 4.58445265701593, Moving Average Reward: -100.41, Epsilon: 0.5134\n",
      "Episode 134 - Total Reward: -105.04282069863433, Moving Average Reward: -101.29, Epsilon: 0.5108\n",
      "Episode 135 - Total Reward: -50.21108896091064, Moving Average Reward: -100.83, Epsilon: 0.5083\n",
      "Episode 136 - Total Reward: -99.16723364413133, Moving Average Reward: -100.40, Epsilon: 0.5058\n",
      "Episode 137 - Total Reward: -64.57956881380099, Moving Average Reward: -99.91, Epsilon: 0.5032\n",
      "Episode 138 - Total Reward: -34.98995481695013, Moving Average Reward: -98.53, Epsilon: 0.5007\n",
      "Episode 139 - Total Reward: -81.50064307739086, Moving Average Reward: -98.24, Epsilon: 0.4982\n",
      "Episode 140 - Total Reward: -69.36258728816637, Moving Average Reward: -96.19, Epsilon: 0.4957\n",
      "Episode 141 - Total Reward: -77.19590598740777, Moving Average Reward: -94.37, Epsilon: 0.4932\n",
      "Episode 142 - Total Reward: 17.04904017742649, Moving Average Reward: -91.90, Epsilon: 0.4908\n",
      "Episode 143 - Total Reward: 10.045949725537128, Moving Average Reward: -90.63, Epsilon: 0.4883\n",
      "Episode 144 - Total Reward: -4.7507949425334175, Moving Average Reward: -89.42, Epsilon: 0.4859\n",
      "Episode 145 - Total Reward: -139.2166676869643, Moving Average Reward: -87.82, Epsilon: 0.4834\n",
      "Episode 146 - Total Reward: -90.89182135306575, Moving Average Reward: -88.69, Epsilon: 0.4810\n",
      "Episode 147 - Total Reward: -42.01817673181082, Moving Average Reward: -87.92, Epsilon: 0.4786\n",
      "Episode 148 - Total Reward: -78.16236384535304, Moving Average Reward: -87.44, Epsilon: 0.4762\n",
      "Episode 149 - Total Reward: -27.7929234597284, Moving Average Reward: -86.16, Epsilon: 0.4738\n",
      "Episode 150 - Total Reward: 25.651938589903594, Moving Average Reward: -85.05, Epsilon: 0.4715\n",
      "Episode 151 - Total Reward: -92.1207772104865, Moving Average Reward: -85.04, Epsilon: 0.4691\n",
      "Episode 152 - Total Reward: -88.88497503742367, Moving Average Reward: -85.04, Epsilon: 0.4668\n",
      "Episode 153 - Total Reward: -14.215615627948395, Moving Average Reward: -84.28, Epsilon: 0.4644\n",
      "Episode 154 - Total Reward: -125.962416612452, Moving Average Reward: -84.91, Epsilon: 0.4621\n",
      "Episode 155 - Total Reward: -129.02211994653942, Moving Average Reward: -85.36, Epsilon: 0.4598\n",
      "Episode 156 - Total Reward: -73.38448427502104, Moving Average Reward: -83.65, Epsilon: 0.4575\n",
      "Episode 157 - Total Reward: -36.73005450845494, Moving Average Reward: -82.94, Epsilon: 0.4552\n",
      "Episode 158 - Total Reward: 20.778311559580885, Moving Average Reward: -81.48, Epsilon: 0.4529\n",
      "Episode 159 - Total Reward: -27.020840598161072, Moving Average Reward: -80.14, Epsilon: 0.4507\n",
      "Episode 160 - Total Reward: -95.10282666349146, Moving Average Reward: -80.06, Epsilon: 0.4484\n",
      "Episode 161 - Total Reward: -53.31546788204179, Moving Average Reward: -78.87, Epsilon: 0.4462\n",
      "Episode 162 - Total Reward: -64.48320193301889, Moving Average Reward: -78.38, Epsilon: 0.4440\n",
      "Episode 163 - Total Reward: -328.78107612785067, Moving Average Reward: -79.63, Epsilon: 0.4417\n",
      "Episode 164 - Total Reward: -16.89335990736312, Moving Average Reward: -79.16, Epsilon: 0.4395\n",
      "Episode 165 - Total Reward: -306.63833955051814, Moving Average Reward: -81.62, Epsilon: 0.4373\n",
      "Episode 166 - Total Reward: -68.62934406166998, Moving Average Reward: -81.43, Epsilon: 0.4351\n",
      "Episode 167 - Total Reward: -8.294608218447536, Moving Average Reward: -80.44, Epsilon: 0.4330\n",
      "Episode 168 - Total Reward: -50.295422335412525, Moving Average Reward: -80.19, Epsilon: 0.4308\n",
      "Episode 169 - Total Reward: -25.65481214502576, Moving Average Reward: -79.42, Epsilon: 0.4286\n",
      "Episode 170 - Total Reward: -133.30652614439708, Moving Average Reward: -80.41, Epsilon: 0.4265\n",
      "Episode 171 - Total Reward: -127.65514614465839, Moving Average Reward: -80.86, Epsilon: 0.4244\n",
      "Episode 172 - Total Reward: -71.8842016512786, Moving Average Reward: -80.86, Epsilon: 0.4223\n",
      "Episode 173 - Total Reward: -43.39228666582916, Moving Average Reward: -80.45, Epsilon: 0.4201\n",
      "Episode 174 - Total Reward: -74.15746440679914, Moving Average Reward: -80.03, Epsilon: 0.4180\n",
      "Episode 175 - Total Reward: -37.21121546182536, Moving Average Reward: -78.28, Epsilon: 0.4159\n",
      "Episode 176 - Total Reward: -181.759171456795, Moving Average Reward: -77.77, Epsilon: 0.4139\n",
      "Episode 177 - Total Reward: -161.48567417439574, Moving Average Reward: -78.90, Epsilon: 0.4118\n",
      "Episode 178 - Total Reward: -16.09357862682289, Moving Average Reward: -78.13, Epsilon: 0.4097\n",
      "Episode 179 - Total Reward: -88.26617406457005, Moving Average Reward: -78.62, Epsilon: 0.4077\n",
      "Episode 180 - Total Reward: -15.82792737943285, Moving Average Reward: -76.46, Epsilon: 0.4057\n",
      "Episode 181 - Total Reward: -0.5901346254273392, Moving Average Reward: -75.10, Epsilon: 0.4036\n",
      "Episode 182 - Total Reward: 33.778571621475095, Moving Average Reward: -74.09, Epsilon: 0.4016\n",
      "Episode 183 - Total Reward: -378.10226819878534, Moving Average Reward: -77.48, Epsilon: 0.3996\n",
      "Episode 184 - Total Reward: -70.88876762850674, Moving Average Reward: -76.99, Epsilon: 0.3976\n",
      "Episode 185 - Total Reward: -122.23727458455657, Moving Average Reward: -77.89, Epsilon: 0.3956\n",
      "Episode 186 - Total Reward: -74.39592885450456, Moving Average Reward: -77.78, Epsilon: 0.3936\n",
      "Episode 187 - Total Reward: 17.74720056824195, Moving Average Reward: -74.65, Epsilon: 0.3917\n",
      "Episode 188 - Total Reward: -186.3436548961035, Moving Average Reward: -75.76, Epsilon: 0.3897\n",
      "Episode 189 - Total Reward: -140.1421134163981, Moving Average Reward: -75.96, Epsilon: 0.3878\n",
      "Episode 190 - Total Reward: -29.264411909979117, Moving Average Reward: -75.42, Epsilon: 0.3858\n",
      "Episode 191 - Total Reward: -73.19102188451745, Moving Average Reward: -75.03, Epsilon: 0.3839\n",
      "Episode 192 - Total Reward: -73.98749826455501, Moving Average Reward: -74.13, Epsilon: 0.3820\n",
      "Episode 193 - Total Reward: -51.10052588807804, Moving Average Reward: -74.22, Epsilon: 0.3801\n",
      "Episode 194 - Total Reward: -104.71008445592318, Moving Average Reward: -74.50, Epsilon: 0.3782\n",
      "Episode 195 - Total Reward: -58.63571659538562, Moving Average Reward: -74.67, Epsilon: 0.3763\n",
      "Episode 196 - Total Reward: -45.081701618244026, Moving Average Reward: -75.18, Epsilon: 0.3744\n",
      "Episode 197 - Total Reward: -25.80207183639456, Moving Average Reward: -74.90, Epsilon: 0.3725\n",
      "Episode 198 - Total Reward: -22.201904723999903, Moving Average Reward: -74.32, Epsilon: 0.3707\n",
      "Episode 199 - Total Reward: -25.110423180202986, Moving Average Reward: -73.88, Epsilon: 0.3688\n",
      "Episode 200 - Total Reward: -45.97445510791876, Moving Average Reward: -73.41, Epsilon: 0.3670\n",
      "Episode 201 - Total Reward: -1707.4430253303062, Moving Average Reward: -89.29, Epsilon: 0.3651\n",
      "Episode 202 - Total Reward: -225.78355724288724, Moving Average Reward: -90.51, Epsilon: 0.3633\n",
      "Episode 203 - Total Reward: -119.57891744512051, Moving Average Reward: -90.98, Epsilon: 0.3615\n",
      "Episode 204 - Total Reward: -122.1891516895262, Moving Average Reward: -91.75, Epsilon: 0.3597\n",
      "Episode 205 - Total Reward: -243.08462249990708, Moving Average Reward: -93.50, Epsilon: 0.3579\n",
      "Episode 206 - Total Reward: 23.957098022633872, Moving Average Reward: -91.82, Epsilon: 0.3561\n",
      "Episode 207 - Total Reward: -280.88677744621396, Moving Average Reward: -94.57, Epsilon: 0.3543\n",
      "Episode 208 - Total Reward: -236.5014656524404, Moving Average Reward: -96.46, Epsilon: 0.3525\n",
      "Episode 209 - Total Reward: -92.39404879284143, Moving Average Reward: -97.20, Epsilon: 0.3508\n",
      "Episode 210 - Total Reward: -199.6500823326689, Moving Average Reward: -98.60, Epsilon: 0.3490\n",
      "Episode 211 - Total Reward: -220.73094207844326, Moving Average Reward: -99.36, Epsilon: 0.3473\n",
      "Episode 212 - Total Reward: -287.83381045126987, Moving Average Reward: -101.71, Epsilon: 0.3455\n",
      "Episode 213 - Total Reward: -193.64929321161867, Moving Average Reward: -102.20, Epsilon: 0.3438\n",
      "Episode 214 - Total Reward: -88.53038209079311, Moving Average Reward: -102.64, Epsilon: 0.3421\n",
      "Episode 215 - Total Reward: -259.0713634884546, Moving Average Reward: -104.77, Epsilon: 0.3404\n",
      "Episode 216 - Total Reward: -16.932086971861352, Moving Average Reward: -105.06, Epsilon: 0.3387\n",
      "Episode 217 - Total Reward: -186.8963285332897, Moving Average Reward: -106.39, Epsilon: 0.3370\n",
      "Episode 218 - Total Reward: -129.11653390666584, Moving Average Reward: -107.52, Epsilon: 0.3353\n",
      "Episode 219 - Total Reward: -286.6414597274214, Moving Average Reward: -110.03, Epsilon: 0.3336\n",
      "Episode 220 - Total Reward: -234.1099512506187, Moving Average Reward: -111.41, Epsilon: 0.3320\n",
      "Episode 221 - Total Reward: -335.46806663212885, Moving Average Reward: -114.53, Epsilon: 0.3303\n",
      "Episode 222 - Total Reward: -207.67726217532174, Moving Average Reward: -115.72, Epsilon: 0.3286\n",
      "Episode 223 - Total Reward: -281.01560391286705, Moving Average Reward: -117.79, Epsilon: 0.3270\n",
      "Episode 224 - Total Reward: -97.3773632868783, Moving Average Reward: -117.28, Epsilon: 0.3254\n",
      "Episode 225 - Total Reward: -199.0847948337311, Moving Average Reward: -118.48, Epsilon: 0.3237\n",
      "Episode 226 - Total Reward: -174.09464928978397, Moving Average Reward: -117.69, Epsilon: 0.3221\n",
      "Episode 227 - Total Reward: -154.72826126286049, Moving Average Reward: -116.99, Epsilon: 0.3205\n",
      "Episode 228 - Total Reward: -203.1923787652575, Moving Average Reward: -118.50, Epsilon: 0.3189\n",
      "Episode 229 - Total Reward: -79.24075507313714, Moving Average Reward: -118.76, Epsilon: 0.3173\n",
      "Episode 230 - Total Reward: -200.88848985423886, Moving Average Reward: -119.87, Epsilon: 0.3157\n",
      "Episode 231 - Total Reward: -488.02360964559927, Moving Average Reward: -124.43, Epsilon: 0.3141\n",
      "Episode 232 - Total Reward: -14.956816920004783, Moving Average Reward: -124.62, Epsilon: 0.3126\n",
      "Episode 233 - Total Reward: -269.65523739575104, Moving Average Reward: -127.37, Epsilon: 0.3110\n",
      "Episode 234 - Total Reward: -217.7181551724204, Moving Average Reward: -128.49, Epsilon: 0.3095\n",
      "Episode 235 - Total Reward: -356.56307476112033, Moving Average Reward: -131.56, Epsilon: 0.3079\n",
      "Episode 236 - Total Reward: -318.6039754398861, Moving Average Reward: -133.75, Epsilon: 0.3064\n",
      "Episode 237 - Total Reward: -163.3422465354671, Moving Average Reward: -134.74, Epsilon: 0.3048\n",
      "Episode 238 - Total Reward: -216.1663675933913, Moving Average Reward: -136.55, Epsilon: 0.3033\n",
      "Episode 239 - Total Reward: -128.61247437877051, Moving Average Reward: -137.02, Epsilon: 0.3018\n",
      "Episode 240 - Total Reward: -137.57544866902236, Moving Average Reward: -137.70, Epsilon: 0.3003\n",
      "Episode 241 - Total Reward: -152.81876968156757, Moving Average Reward: -138.46, Epsilon: 0.2988\n",
      "Episode 242 - Total Reward: -353.9720341998276, Moving Average Reward: -142.17, Epsilon: 0.2973\n",
      "Episode 243 - Total Reward: -262.3373977915427, Moving Average Reward: -144.89, Epsilon: 0.2958\n",
      "Episode 244 - Total Reward: -287.67546298029686, Moving Average Reward: -147.72, Epsilon: 0.2943\n",
      "Episode 245 - Total Reward: -260.6214009833343, Moving Average Reward: -148.94, Epsilon: 0.2929\n",
      "Episode 246 - Total Reward: -194.63627610421435, Moving Average Reward: -149.97, Epsilon: 0.2914\n",
      "Episode 247 - Total Reward: -198.0820659934235, Moving Average Reward: -151.53, Epsilon: 0.2899\n",
      "Episode 248 - Total Reward: -96.94420241432809, Moving Average Reward: -151.72, Epsilon: 0.2885\n",
      "Episode 249 - Total Reward: 23.963784912341836, Moving Average Reward: -151.20, Epsilon: 0.2870\n",
      "Episode 250 - Total Reward: -179.756960655338, Moving Average Reward: -153.26, Epsilon: 0.2856\n",
      "Episode 251 - Total Reward: -80.9569181556208, Moving Average Reward: -153.15, Epsilon: 0.2842\n",
      "Episode 252 - Total Reward: -167.01197455956049, Moving Average Reward: -153.93, Epsilon: 0.2828\n",
      "Episode 253 - Total Reward: -183.22671488179213, Moving Average Reward: -155.62, Epsilon: 0.2813\n",
      "Episode 254 - Total Reward: -258.40199169074856, Moving Average Reward: -156.94, Epsilon: 0.2799\n",
      "Episode 255 - Total Reward: -170.77094464363034, Moving Average Reward: -157.36, Epsilon: 0.2785\n",
      "Episode 256 - Total Reward: -192.6225160143937, Moving Average Reward: -158.55, Epsilon: 0.2771\n",
      "Episode 257 - Total Reward: -199.19944360632542, Moving Average Reward: -160.18, Epsilon: 0.2758\n",
      "Episode 258 - Total Reward: -215.99954947625693, Moving Average Reward: -162.55, Epsilon: 0.2744\n",
      "Episode 259 - Total Reward: -173.07498478352665, Moving Average Reward: -164.01, Epsilon: 0.2730\n",
      "Episode 260 - Total Reward: -278.95944677219615, Moving Average Reward: -165.84, Epsilon: 0.2716\n",
      "Episode 261 - Total Reward: -188.43164383761604, Moving Average Reward: -167.20, Epsilon: 0.2703\n",
      "Episode 262 - Total Reward: -143.5982187201535, Moving Average Reward: -167.99, Epsilon: 0.2689\n",
      "Episode 263 - Total Reward: -309.8057478689053, Moving Average Reward: -167.80, Epsilon: 0.2676\n",
      "Episode 264 - Total Reward: -161.69116600721446, Moving Average Reward: -169.24, Epsilon: 0.2663\n",
      "Episode 265 - Total Reward: -137.36465715306196, Moving Average Reward: -167.55, Epsilon: 0.2649\n",
      "Episode 266 - Total Reward: -331.73973089686115, Moving Average Reward: -170.18, Epsilon: 0.2636\n",
      "Episode 267 - Total Reward: -364.08131037501477, Moving Average Reward: -173.74, Epsilon: 0.2623\n",
      "Episode 268 - Total Reward: -341.4464880703109, Moving Average Reward: -176.65, Epsilon: 0.2610\n",
      "Episode 269 - Total Reward: -166.4308629900733, Moving Average Reward: -178.06, Epsilon: 0.2597\n",
      "Episode 270 - Total Reward: -1.6811691934803008, Moving Average Reward: -176.74, Epsilon: 0.2584\n",
      "Episode 271 - Total Reward: -205.79898684915085, Moving Average Reward: -177.53, Epsilon: 0.2571\n",
      "Episode 272 - Total Reward: -68.97236695205626, Moving Average Reward: -177.50, Epsilon: 0.2558\n",
      "Episode 273 - Total Reward: -353.6184158684247, Moving Average Reward: -180.60, Epsilon: 0.2545\n",
      "Episode 274 - Total Reward: -156.45276890151086, Moving Average Reward: -181.42, Epsilon: 0.2532\n",
      "Episode 275 - Total Reward: -137.78250644428448, Moving Average Reward: -182.43, Epsilon: 0.2520\n",
      "Episode 276 - Total Reward: -249.7378279529198, Moving Average Reward: -183.11, Epsilon: 0.2507\n",
      "Episode 277 - Total Reward: -245.1252176028138, Moving Average Reward: -183.94, Epsilon: 0.2495\n",
      "Episode 278 - Total Reward: -230.60343365207774, Moving Average Reward: -186.09, Epsilon: 0.2482\n",
      "Episode 279 - Total Reward: -150.48548645943254, Moving Average Reward: -186.71, Epsilon: 0.2470\n",
      "Episode 280 - Total Reward: -180.0556229863818, Moving Average Reward: -188.35, Epsilon: 0.2457\n",
      "Episode 281 - Total Reward: -317.90352903514236, Moving Average Reward: -191.53, Epsilon: 0.2445\n",
      "Episode 282 - Total Reward: -306.41232091174186, Moving Average Reward: -194.93, Epsilon: 0.2433\n",
      "Episode 283 - Total Reward: -4.255305993564065, Moving Average Reward: -191.19, Epsilon: 0.2421\n",
      "Episode 284 - Total Reward: -90.66463497355642, Moving Average Reward: -191.39, Epsilon: 0.2409\n",
      "Episode 285 - Total Reward: -260.33506936702634, Moving Average Reward: -192.77, Epsilon: 0.2397\n",
      "Episode 286 - Total Reward: -204.2011132124442, Moving Average Reward: -194.07, Epsilon: 0.2385\n",
      "Episode 287 - Total Reward: -78.20342860155759, Moving Average Reward: -195.03, Epsilon: 0.2373\n",
      "Episode 288 - Total Reward: -142.32238837477013, Moving Average Reward: -194.59, Epsilon: 0.2361\n",
      "Episode 289 - Total Reward: -143.64429328449705, Moving Average Reward: -194.62, Epsilon: 0.2349\n",
      "Episode 290 - Total Reward: -147.98360926449877, Moving Average Reward: -195.81, Epsilon: 0.2337\n",
      "Episode 291 - Total Reward: -197.27047625361672, Moving Average Reward: -197.05, Epsilon: 0.2326\n",
      "Episode 292 - Total Reward: -152.35227704756716, Moving Average Reward: -197.83, Epsilon: 0.2314\n",
      "Episode 293 - Total Reward: -176.8405299812333, Moving Average Reward: -199.09, Epsilon: 0.2302\n",
      "Episode 294 - Total Reward: -191.91698870399972, Moving Average Reward: -199.96, Epsilon: 0.2291\n",
      "Episode 295 - Total Reward: -228.59147646428502, Moving Average Reward: -201.66, Epsilon: 0.2279\n",
      "Episode 296 - Total Reward: -288.53015669621595, Moving Average Reward: -204.10, Epsilon: 0.2268\n",
      "Episode 297 - Total Reward: -73.91212820233113, Moving Average Reward: -204.58, Epsilon: 0.2257\n",
      "Episode 298 - Total Reward: -204.2719206984562, Moving Average Reward: -206.40, Epsilon: 0.2245\n",
      "Episode 299 - Total Reward: -144.56174881848204, Moving Average Reward: -207.59, Epsilon: 0.2234\n",
      "Episode 300 - Total Reward: -222.9033095500497, Moving Average Reward: -209.36, Epsilon: 0.2223\n",
      "Episode 301 - Total Reward: -81.04942795974628, Moving Average Reward: -193.10, Epsilon: 0.2212\n",
      "Episode 302 - Total Reward: -370.3781244604233, Moving Average Reward: -194.54, Epsilon: 0.2201\n",
      "Episode 303 - Total Reward: -309.0742626508337, Moving Average Reward: -196.44, Epsilon: 0.2190\n",
      "Episode 304 - Total Reward: -35.01551908044662, Moving Average Reward: -195.57, Epsilon: 0.2179\n",
      "Episode 305 - Total Reward: -214.95037634613968, Moving Average Reward: -195.29, Epsilon: 0.2168\n",
      "Episode 306 - Total Reward: -217.40669748934596, Moving Average Reward: -197.70, Epsilon: 0.2157\n",
      "Episode 307 - Total Reward: -150.4534043198312, Moving Average Reward: -196.39, Epsilon: 0.2146\n",
      "Episode 308 - Total Reward: -302.14380597047585, Moving Average Reward: -197.05, Epsilon: 0.2136\n",
      "Episode 309 - Total Reward: -112.16491135789443, Moving Average Reward: -197.25, Epsilon: 0.2125\n",
      "Episode 310 - Total Reward: -95.82420719869418, Moving Average Reward: -196.21, Epsilon: 0.2114\n",
      "Episode 311 - Total Reward: -229.32257275989838, Moving Average Reward: -196.30, Epsilon: 0.2104\n",
      "Episode 312 - Total Reward: -248.97378629313857, Moving Average Reward: -195.91, Epsilon: 0.2093\n",
      "Episode 313 - Total Reward: -241.90233801216903, Moving Average Reward: -196.39, Epsilon: 0.2083\n",
      "Episode 314 - Total Reward: 11.655721256357168, Moving Average Reward: -195.39, Epsilon: 0.2072\n",
      "Episode 315 - Total Reward: -206.49949866187052, Moving Average Reward: -194.86, Epsilon: 0.2062\n",
      "Episode 316 - Total Reward: -251.63502723634465, Moving Average Reward: -197.21, Epsilon: 0.2052\n",
      "Episode 317 - Total Reward: -221.56983687192923, Moving Average Reward: -197.56, Epsilon: 0.2041\n",
      "Episode 318 - Total Reward: -227.00523497880772, Moving Average Reward: -198.54, Epsilon: 0.2031\n",
      "Episode 319 - Total Reward: -259.21790052875747, Moving Average Reward: -198.26, Epsilon: 0.2021\n",
      "Episode 320 - Total Reward: -101.93594564750819, Moving Average Reward: -196.94, Epsilon: 0.2011\n",
      "Episode 321 - Total Reward: -191.0324943209768, Moving Average Reward: -195.50, Epsilon: 0.2001\n",
      "Episode 322 - Total Reward: -253.2515976757847, Moving Average Reward: -195.95, Epsilon: 0.1991\n",
      "Episode 323 - Total Reward: -133.4731555253374, Moving Average Reward: -194.48, Epsilon: 0.1981\n",
      "Episode 324 - Total Reward: -344.2913271291783, Moving Average Reward: -196.94, Epsilon: 0.1971\n",
      "Episode 325 - Total Reward: -23.329447927498094, Moving Average Reward: -195.19, Epsilon: 0.1961\n",
      "Episode 326 - Total Reward: -203.98248825599376, Moving Average Reward: -195.49, Epsilon: 0.1951\n",
      "Episode 327 - Total Reward: -335.59855253313606, Moving Average Reward: -197.29, Epsilon: 0.1942\n",
      "Episode 328 - Total Reward: -131.9898868658255, Moving Average Reward: -196.58, Epsilon: 0.1932\n",
      "Episode 329 - Total Reward: -324.50325708908775, Moving Average Reward: -199.04, Epsilon: 0.1922\n",
      "Episode 330 - Total Reward: -15.342881309112443, Moving Average Reward: -197.18, Epsilon: 0.1913\n",
      "Episode 331 - Total Reward: -127.76976209677221, Moving Average Reward: -193.58, Epsilon: 0.1903\n",
      "Episode 332 - Total Reward: -100.49143306972384, Moving Average Reward: -194.43, Epsilon: 0.1893\n",
      "Episode 333 - Total Reward: -236.26360950099163, Moving Average Reward: -194.10, Epsilon: 0.1884\n",
      "Episode 334 - Total Reward: -106.51096063920362, Moving Average Reward: -192.99, Epsilon: 0.1875\n",
      "Episode 335 - Total Reward: -340.69151872340484, Moving Average Reward: -192.83, Epsilon: 0.1865\n",
      "Episode 336 - Total Reward: -136.21111141382883, Moving Average Reward: -191.00, Epsilon: 0.1856\n",
      "Episode 337 - Total Reward: -126.25880068585995, Moving Average Reward: -190.63, Epsilon: 0.1847\n",
      "Episode 338 - Total Reward: -579.9316661782234, Moving Average Reward: -194.27, Epsilon: 0.1837\n",
      "Episode 339 - Total Reward: -170.44450145139993, Moving Average Reward: -194.69, Epsilon: 0.1828\n",
      "Episode 340 - Total Reward: -333.5778622460672, Moving Average Reward: -196.65, Epsilon: 0.1819\n",
      "Episode 341 - Total Reward: -179.33458038674428, Moving Average Reward: -196.91, Epsilon: 0.1810\n",
      "Episode 342 - Total Reward: -349.3035635589474, Moving Average Reward: -196.87, Epsilon: 0.1801\n",
      "Episode 343 - Total Reward: 247.8146831828095, Moving Average Reward: -191.77, Epsilon: 0.1792\n",
      "Episode 344 - Total Reward: -370.0397701175274, Moving Average Reward: -192.59, Epsilon: 0.1783\n",
      "Episode 345 - Total Reward: -349.11858869165803, Moving Average Reward: -193.47, Epsilon: 0.1774\n",
      "Episode 346 - Total Reward: -67.80855261600306, Moving Average Reward: -192.21, Epsilon: 0.1765\n",
      "Episode 347 - Total Reward: -448.5720656627232, Moving Average Reward: -194.71, Epsilon: 0.1756\n",
      "Episode 348 - Total Reward: -71.60027453824125, Moving Average Reward: -194.46, Epsilon: 0.1748\n",
      "Episode 349 - Total Reward: -167.4029541865157, Moving Average Reward: -196.37, Epsilon: 0.1739\n",
      "Episode 350 - Total Reward: -78.08980339065096, Moving Average Reward: -195.35, Epsilon: 0.1730\n",
      "Episode 351 - Total Reward: -151.57403354613498, Moving Average Reward: -196.06, Epsilon: 0.1721\n",
      "Episode 352 - Total Reward: -210.9428662995266, Moving Average Reward: -196.50, Epsilon: 0.1713\n",
      "Episode 353 - Total Reward: -148.70993373257406, Moving Average Reward: -196.16, Epsilon: 0.1704\n",
      "Episode 354 - Total Reward: -198.3723315912133, Moving Average Reward: -195.55, Epsilon: 0.1696\n",
      "Episode 355 - Total Reward: -196.2581201231486, Moving Average Reward: -195.81, Epsilon: 0.1687\n",
      "Episode 356 - Total Reward: -215.31809936899185, Moving Average Reward: -196.04, Epsilon: 0.1679\n",
      "Episode 357 - Total Reward: -254.67646349344952, Moving Average Reward: -196.59, Epsilon: 0.1670\n",
      "Episode 358 - Total Reward: -231.72818448763584, Moving Average Reward: -196.75, Epsilon: 0.1662\n",
      "Episode 359 - Total Reward: -37.21843734210562, Moving Average Reward: -195.39, Epsilon: 0.1654\n",
      "Episode 360 - Total Reward: -92.89192154472849, Moving Average Reward: -193.53, Epsilon: 0.1646\n",
      "Episode 361 - Total Reward: -85.49670310205585, Moving Average Reward: -192.50, Epsilon: 0.1637\n",
      "Episode 362 - Total Reward: -10.095983300347527, Moving Average Reward: -191.17, Epsilon: 0.1629\n",
      "Episode 363 - Total Reward: -296.15182838220403, Moving Average Reward: -191.03, Epsilon: 0.1621\n",
      "Episode 364 - Total Reward: -185.00307039874886, Moving Average Reward: -191.26, Epsilon: 0.1613\n",
      "Episode 365 - Total Reward: -200.81336811307705, Moving Average Reward: -191.90, Epsilon: 0.1605\n",
      "Episode 366 - Total Reward: -257.5786127568667, Moving Average Reward: -191.15, Epsilon: 0.1597\n",
      "Episode 367 - Total Reward: -324.95826363484696, Moving Average Reward: -190.76, Epsilon: 0.1589\n",
      "Episode 368 - Total Reward: -155.92528083523578, Moving Average Reward: -188.91, Epsilon: 0.1581\n",
      "Episode 369 - Total Reward: -26.434748135513956, Moving Average Reward: -187.51, Epsilon: 0.1573\n",
      "Episode 370 - Total Reward: -410.52668930868595, Moving Average Reward: -191.60, Epsilon: 0.1565\n",
      "Episode 371 - Total Reward: -288.51128998187687, Moving Average Reward: -192.42, Epsilon: 0.1557\n",
      "Episode 372 - Total Reward: -231.30234867309952, Moving Average Reward: -194.05, Epsilon: 0.1549\n",
      "Episode 373 - Total Reward: -119.83668842985641, Moving Average Reward: -191.71, Epsilon: 0.1542\n",
      "Episode 374 - Total Reward: -215.56714827153093, Moving Average Reward: -192.30, Epsilon: 0.1534\n",
      "Episode 375 - Total Reward: -174.27646439940793, Moving Average Reward: -192.67, Epsilon: 0.1526\n",
      "Episode 376 - Total Reward: -266.2512229528784, Moving Average Reward: -192.83, Epsilon: 0.1519\n",
      "Episode 377 - Total Reward: -169.0538207131352, Moving Average Reward: -192.07, Epsilon: 0.1511\n",
      "Episode 378 - Total Reward: -225.57963393678304, Moving Average Reward: -192.02, Epsilon: 0.1504\n",
      "Episode 379 - Total Reward: -287.5556508002535, Moving Average Reward: -193.39, Epsilon: 0.1496\n",
      "Episode 380 - Total Reward: -284.228865760555, Moving Average Reward: -194.43, Epsilon: 0.1489\n",
      "Episode 381 - Total Reward: -250.19113710480605, Moving Average Reward: -193.75, Epsilon: 0.1481\n",
      "Episode 382 - Total Reward: -14.262456346306408, Moving Average Reward: -190.83, Epsilon: 0.1474\n",
      "Episode 383 - Total Reward: -166.5912226501069, Moving Average Reward: -192.46, Epsilon: 0.1466\n",
      "Episode 384 - Total Reward: -342.1922372156582, Moving Average Reward: -194.97, Epsilon: 0.1459\n",
      "Episode 385 - Total Reward: -484.42711554058235, Moving Average Reward: -197.21, Epsilon: 0.1452\n",
      "Episode 386 - Total Reward: -347.48828192862453, Moving Average Reward: -198.65, Epsilon: 0.1444\n",
      "Episode 387 - Total Reward: -121.6826409471769, Moving Average Reward: -199.08, Epsilon: 0.1437\n",
      "Episode 388 - Total Reward: -67.81511957665744, Moving Average Reward: -198.34, Epsilon: 0.1430\n",
      "Episode 389 - Total Reward: -180.4613521285105, Moving Average Reward: -198.70, Epsilon: 0.1423\n",
      "Episode 390 - Total Reward: -226.85411866210796, Moving Average Reward: -199.49, Epsilon: 0.1416\n",
      "Episode 391 - Total Reward: -428.57444602916337, Moving Average Reward: -201.81, Epsilon: 0.1409\n",
      "Episode 392 - Total Reward: -39.70247080748605, Moving Average Reward: -200.68, Epsilon: 0.1402\n",
      "Episode 393 - Total Reward: -406.10964024695676, Moving Average Reward: -202.97, Epsilon: 0.1395\n",
      "Episode 394 - Total Reward: -292.65096998696646, Moving Average Reward: -203.98, Epsilon: 0.1388\n",
      "Episode 395 - Total Reward: -38.974921968587864, Moving Average Reward: -202.08, Epsilon: 0.1381\n",
      "Episode 396 - Total Reward: -79.70974181256126, Moving Average Reward: -199.99, Epsilon: 0.1374\n",
      "Episode 397 - Total Reward: -307.62086085822057, Moving Average Reward: -202.33, Epsilon: 0.1367\n",
      "Episode 398 - Total Reward: -174.62764704374936, Moving Average Reward: -202.04, Epsilon: 0.1360\n",
      "Episode 399 - Total Reward: -178.36939131660012, Moving Average Reward: -202.37, Epsilon: 0.1353\n",
      "Episode 400 - Total Reward: -221.3651468032938, Moving Average Reward: -202.36, Epsilon: 0.1347\n",
      "Episode 401 - Total Reward: -255.16826656495644, Moving Average Reward: -204.10, Epsilon: 0.1340\n",
      "Episode 402 - Total Reward: -308.0208866432383, Moving Average Reward: -203.48, Epsilon: 0.1333\n",
      "Episode 403 - Total Reward: -201.48614882813854, Moving Average Reward: -202.40, Epsilon: 0.1326\n",
      "Episode 404 - Total Reward: -278.9031507825149, Moving Average Reward: -204.84, Epsilon: 0.1320\n",
      "Episode 405 - Total Reward: -253.51749610056123, Moving Average Reward: -205.22, Epsilon: 0.1313\n",
      "Episode 406 - Total Reward: -219.1186138778271, Moving Average Reward: -205.24, Epsilon: 0.1307\n",
      "Episode 407 - Total Reward: -150.9017089055962, Moving Average Reward: -205.25, Epsilon: 0.1300\n",
      "Episode 408 - Total Reward: -190.07154484479665, Moving Average Reward: -204.12, Epsilon: 0.1294\n",
      "Episode 409 - Total Reward: -303.1320957896237, Moving Average Reward: -206.03, Epsilon: 0.1287\n",
      "Episode 410 - Total Reward: -398.4860842334574, Moving Average Reward: -209.06, Epsilon: 0.1281\n",
      "Episode 411 - Total Reward: -304.7516866542716, Moving Average Reward: -209.82, Epsilon: 0.1274\n",
      "Episode 412 - Total Reward: -131.69929961282307, Moving Average Reward: -208.64, Epsilon: 0.1268\n",
      "Episode 413 - Total Reward: -111.53523536144453, Moving Average Reward: -207.34, Epsilon: 0.1262\n",
      "Episode 414 - Total Reward: -76.89926842789887, Moving Average Reward: -208.22, Epsilon: 0.1255\n",
      "Episode 415 - Total Reward: -189.28726324578776, Moving Average Reward: -208.05, Epsilon: 0.1249\n",
      "Episode 416 - Total Reward: -274.62942127390784, Moving Average Reward: -208.28, Epsilon: 0.1243\n",
      "Episode 417 - Total Reward: -145.96623624809982, Moving Average Reward: -207.53, Epsilon: 0.1237\n",
      "Episode 418 - Total Reward: -213.15970472481888, Moving Average Reward: -207.39, Epsilon: 0.1230\n",
      "Episode 419 - Total Reward: -268.3641245458017, Moving Average Reward: -207.48, Epsilon: 0.1224\n",
      "Episode 420 - Total Reward: -212.97477707042918, Moving Average Reward: -208.59, Epsilon: 0.1218\n",
      "Episode 421 - Total Reward: -105.96807611530642, Moving Average Reward: -207.74, Epsilon: 0.1212\n",
      "Episode 422 - Total Reward: -87.06742059747188, Moving Average Reward: -206.08, Epsilon: 0.1206\n",
      "Episode 423 - Total Reward: -200.08715467153485, Moving Average Reward: -206.74, Epsilon: 0.1200\n",
      "Episode 424 - Total Reward: -84.70324243602134, Moving Average Reward: -204.15, Epsilon: 0.1194\n",
      "Episode 425 - Total Reward: -438.2169979467093, Moving Average Reward: -208.30, Epsilon: 0.1188\n",
      "Episode 426 - Total Reward: -179.47520936927302, Moving Average Reward: -208.05, Epsilon: 0.1182\n",
      "Episode 427 - Total Reward: -250.1415100159437, Moving Average Reward: -207.20, Epsilon: 0.1176\n",
      "Episode 428 - Total Reward: -248.2418739353909, Moving Average Reward: -208.36, Epsilon: 0.1170\n",
      "Episode 429 - Total Reward: -522.7247453754862, Moving Average Reward: -210.34, Epsilon: 0.1164\n",
      "Episode 430 - Total Reward: -156.19104519766233, Moving Average Reward: -211.75, Epsilon: 0.1159\n",
      "Episode 431 - Total Reward: -113.74094605276498, Moving Average Reward: -211.61, Epsilon: 0.1153\n",
      "Episode 432 - Total Reward: -177.72387771750286, Moving Average Reward: -212.38, Epsilon: 0.1147\n",
      "Episode 433 - Total Reward: -131.00159900741008, Moving Average Reward: -211.33, Epsilon: 0.1141\n",
      "Episode 434 - Total Reward: -291.3251897653593, Moving Average Reward: -213.18, Epsilon: 0.1136\n",
      "Episode 435 - Total Reward: 28.56348134728239, Moving Average Reward: -209.49, Epsilon: 0.1130\n",
      "Episode 436 - Total Reward: 12.528150880127555, Moving Average Reward: -208.00, Epsilon: 0.1124\n",
      "Episode 437 - Total Reward: -295.1879725926887, Moving Average Reward: -209.69, Epsilon: 0.1119\n",
      "Episode 438 - Total Reward: -79.54121661981193, Moving Average Reward: -204.68, Epsilon: 0.1113\n",
      "Episode 439 - Total Reward: -121.65458543203815, Moving Average Reward: -204.20, Epsilon: 0.1107\n",
      "Episode 440 - Total Reward: -241.61180329360272, Moving Average Reward: -203.28, Epsilon: 0.1102\n",
      "Episode 441 - Total Reward: -158.03166508497097, Moving Average Reward: -203.06, Epsilon: 0.1096\n",
      "Episode 442 - Total Reward: -51.439760424534406, Moving Average Reward: -200.08, Epsilon: 0.1091\n",
      "Episode 443 - Total Reward: -188.43853688512894, Moving Average Reward: -204.45, Epsilon: 0.1085\n",
      "Episode 444 - Total Reward: -241.1544999417856, Moving Average Reward: -203.16, Epsilon: 0.1080\n",
      "Episode 445 - Total Reward: -382.4394671464111, Moving Average Reward: -203.49, Epsilon: 0.1075\n",
      "Episode 446 - Total Reward: -275.652607196974, Moving Average Reward: -205.57, Epsilon: 0.1069\n",
      "Episode 447 - Total Reward: -129.5896752893704, Moving Average Reward: -202.38, Epsilon: 0.1064\n",
      "Episode 448 - Total Reward: -199.13062765214676, Moving Average Reward: -203.65, Epsilon: 0.1059\n",
      "Episode 449 - Total Reward: -46.841100779735584, Moving Average Reward: -202.45, Epsilon: 0.1053\n",
      "Episode 450 - Total Reward: -259.1794004826977, Moving Average Reward: -204.26, Epsilon: 0.1048\n",
      "Episode 451 - Total Reward: -454.45224448362853, Moving Average Reward: -207.29, Epsilon: 0.1043\n",
      "Episode 452 - Total Reward: -39.366511568954834, Moving Average Reward: -205.57, Epsilon: 0.1038\n",
      "Episode 453 - Total Reward: -154.8101183912965, Moving Average Reward: -205.63, Epsilon: 0.1032\n",
      "Episode 454 - Total Reward: -298.06210559102533, Moving Average Reward: -206.63, Epsilon: 0.1027\n",
      "Episode 455 - Total Reward: -243.35212786468426, Moving Average Reward: -207.10, Epsilon: 0.1022\n",
      "Episode 456 - Total Reward: -157.80902846687798, Moving Average Reward: -206.53, Epsilon: 0.1017\n",
      "Episode 457 - Total Reward: -119.09065701531188, Moving Average Reward: -205.17, Epsilon: 0.1012\n",
      "Episode 458 - Total Reward: -423.0816980987437, Moving Average Reward: -207.08, Epsilon: 0.1007\n",
      "Episode 459 - Total Reward: -235.03543198035837, Moving Average Reward: -209.06, Epsilon: 0.1002\n",
      "Episode 460 - Total Reward: -451.1313511394478, Moving Average Reward: -212.64, Epsilon: 0.0997\n",
      "Episode 461 - Total Reward: -199.83300084928317, Moving Average Reward: -213.79, Epsilon: 0.0992\n",
      "Episode 462 - Total Reward: -229.08447445777173, Moving Average Reward: -215.98, Epsilon: 0.0987\n",
      "Episode 463 - Total Reward: -245.33560496345427, Moving Average Reward: -215.47, Epsilon: 0.0982\n",
      "Episode 464 - Total Reward: -427.1890129681089, Moving Average Reward: -217.89, Epsilon: 0.0977\n",
      "Episode 465 - Total Reward: 18.196114957506026, Moving Average Reward: -215.70, Epsilon: 0.0972\n",
      "Episode 466 - Total Reward: -467.664876180736, Moving Average Reward: -217.80, Epsilon: 0.0967\n",
      "Episode 467 - Total Reward: -220.75235473451744, Moving Average Reward: -216.76, Epsilon: 0.0962\n",
      "Episode 468 - Total Reward: -158.86746161065557, Moving Average Reward: -216.79, Epsilon: 0.0958\n",
      "Episode 469 - Total Reward: -65.01644154155548, Moving Average Reward: -217.18, Epsilon: 0.0953\n",
      "Episode 470 - Total Reward: -187.46712218461647, Moving Average Reward: -214.95, Epsilon: 0.0948\n",
      "Episode 471 - Total Reward: 27.236530447039584, Moving Average Reward: -211.79, Epsilon: 0.0943\n",
      "Episode 472 - Total Reward: -455.48795261160785, Moving Average Reward: -214.03, Epsilon: 0.0939\n",
      "Episode 473 - Total Reward: -155.71118578310598, Moving Average Reward: -214.39, Epsilon: 0.0934\n",
      "Episode 474 - Total Reward: -319.98912850709655, Moving Average Reward: -215.43, Epsilon: 0.0929\n",
      "Episode 475 - Total Reward: -439.3669790325307, Moving Average Reward: -218.08, Epsilon: 0.0925\n",
      "Episode 476 - Total Reward: -0.5750223621398476, Moving Average Reward: -215.43, Epsilon: 0.0920\n",
      "Episode 477 - Total Reward: -252.56627353152965, Moving Average Reward: -216.26, Epsilon: 0.0915\n",
      "Episode 478 - Total Reward: -127.43192632148589, Moving Average Reward: -215.28, Epsilon: 0.0911\n",
      "Episode 479 - Total Reward: -147.07811218050702, Moving Average Reward: -213.88, Epsilon: 0.0906\n",
      "Episode 480 - Total Reward: -377.95917076459534, Moving Average Reward: -214.81, Epsilon: 0.0902\n",
      "Episode 481 - Total Reward: -227.5317033655869, Moving Average Reward: -214.59, Epsilon: 0.0897\n",
      "Episode 482 - Total Reward: -276.0151351308696, Moving Average Reward: -217.20, Epsilon: 0.0893\n",
      "Episode 483 - Total Reward: -393.4025296375824, Moving Average Reward: -219.47, Epsilon: 0.0888\n",
      "Episode 484 - Total Reward: -165.41718703770994, Moving Average Reward: -217.70, Epsilon: 0.0884\n",
      "Episode 485 - Total Reward: -217.4437816593196, Moving Average Reward: -215.03, Epsilon: 0.0879\n",
      "Episode 486 - Total Reward: -415.56547943233863, Moving Average Reward: -215.72, Epsilon: 0.0875\n",
      "Episode 487 - Total Reward: -176.7453928768021, Moving Average Reward: -216.27, Epsilon: 0.0871\n",
      "Episode 488 - Total Reward: -128.97703399818351, Moving Average Reward: -216.88, Epsilon: 0.0866\n",
      "Episode 489 - Total Reward: -322.2235468763331, Moving Average Reward: -218.29, Epsilon: 0.0862\n",
      "Episode 490 - Total Reward: -269.21799706093157, Moving Average Reward: -218.72, Epsilon: 0.0858\n",
      "Episode 491 - Total Reward: -469.747331884873, Moving Average Reward: -219.13, Epsilon: 0.0853\n",
      "Episode 492 - Total Reward: -379.62069849725435, Moving Average Reward: -222.53, Epsilon: 0.0849\n",
      "Episode 493 - Total Reward: -141.52505690650148, Moving Average Reward: -219.88, Epsilon: 0.0845\n",
      "Episode 494 - Total Reward: -441.80371782492125, Moving Average Reward: -221.38, Epsilon: 0.0841\n",
      "Episode 495 - Total Reward: -455.44705834552764, Moving Average Reward: -225.54, Epsilon: 0.0836\n",
      "Episode 496 - Total Reward: -455.99105601782213, Moving Average Reward: -229.30, Epsilon: 0.0832\n",
      "Episode 497 - Total Reward: -462.3433012412871, Moving Average Reward: -230.85, Epsilon: 0.0828\n",
      "Episode 498 - Total Reward: -255.34420525726122, Moving Average Reward: -231.66, Epsilon: 0.0824\n",
      "Episode 499 - Total Reward: -244.0830016740445, Moving Average Reward: -232.31, Epsilon: 0.0820\n",
      "Episode 500 - Total Reward: -80.10301044186315, Moving Average Reward: -230.90, Epsilon: 0.0816\n",
      "Episode 501 - Total Reward: -204.973195258747, Moving Average Reward: -230.40, Epsilon: 0.0812\n",
      "Episode 502 - Total Reward: -369.72744302526036, Moving Average Reward: -231.02, Epsilon: 0.0808\n",
      "Episode 503 - Total Reward: -311.328718109864, Moving Average Reward: -232.12, Epsilon: 0.0804\n",
      "Episode 504 - Total Reward: -422.6749040165613, Moving Average Reward: -233.55, Epsilon: 0.0800\n",
      "Episode 505 - Total Reward: -259.7109241442628, Moving Average Reward: -233.61, Epsilon: 0.0796\n",
      "Episode 506 - Total Reward: -250.6340326219002, Moving Average Reward: -233.93, Epsilon: 0.0792\n",
      "Episode 507 - Total Reward: -261.65415635869067, Moving Average Reward: -235.04, Epsilon: 0.0788\n",
      "Episode 508 - Total Reward: -489.8751057943905, Moving Average Reward: -238.04, Epsilon: 0.0784\n",
      "Episode 509 - Total Reward: -353.48892450388064, Moving Average Reward: -238.54, Epsilon: 0.0780\n",
      "Episode 510 - Total Reward: -184.07052839777975, Moving Average Reward: -236.39, Epsilon: 0.0776\n",
      "Episode 511 - Total Reward: -358.3257406115969, Moving Average Reward: -236.93, Epsilon: 0.0772\n",
      "Episode 512 - Total Reward: -133.5359074107347, Moving Average Reward: -236.95, Epsilon: 0.0768\n",
      "Episode 513 - Total Reward: -267.22407185070097, Moving Average Reward: -238.51, Epsilon: 0.0764\n",
      "Episode 514 - Total Reward: -237.8432912255341, Moving Average Reward: -240.12, Epsilon: 0.0760\n",
      "Episode 515 - Total Reward: -325.28449898901977, Moving Average Reward: -241.48, Epsilon: 0.0757\n",
      "Episode 516 - Total Reward: -109.38444080832359, Moving Average Reward: -239.82, Epsilon: 0.0753\n",
      "Episode 517 - Total Reward: -337.1266056585361, Moving Average Reward: -241.73, Epsilon: 0.0749\n",
      "Episode 518 - Total Reward: -140.65807629811923, Moving Average Reward: -241.01, Epsilon: 0.0745\n",
      "Episode 519 - Total Reward: -303.05017554285854, Moving Average Reward: -241.36, Epsilon: 0.0742\n",
      "Episode 520 - Total Reward: -306.9951190916468, Moving Average Reward: -242.30, Epsilon: 0.0738\n",
      "Episode 521 - Total Reward: -163.96246880404235, Moving Average Reward: -242.88, Epsilon: 0.0734\n",
      "Episode 522 - Total Reward: -413.54391739731295, Moving Average Reward: -246.14, Epsilon: 0.0731\n",
      "Episode 523 - Total Reward: -399.7409900767685, Moving Average Reward: -248.14, Epsilon: 0.0727\n",
      "Episode 524 - Total Reward: -268.6520801529406, Moving Average Reward: -249.98, Epsilon: 0.0723\n",
      "Episode 525 - Total Reward: -368.20985652037996, Moving Average Reward: -249.28, Epsilon: 0.0720\n",
      "Episode 526 - Total Reward: -276.89376014463176, Moving Average Reward: -250.25, Epsilon: 0.0716\n",
      "Episode 527 - Total Reward: -168.7373908302514, Moving Average Reward: -249.44, Epsilon: 0.0712\n",
      "Episode 528 - Total Reward: -269.831651441027, Moving Average Reward: -249.65, Epsilon: 0.0709\n",
      "Episode 529 - Total Reward: -131.01965961071508, Moving Average Reward: -245.74, Epsilon: 0.0705\n",
      "Episode 530 - Total Reward: -205.74861700623654, Moving Average Reward: -246.23, Epsilon: 0.0702\n",
      "Episode 531 - Total Reward: -180.18818818430123, Moving Average Reward: -246.90, Epsilon: 0.0698\n",
      "Episode 532 - Total Reward: -430.0272015004979, Moving Average Reward: -249.42, Epsilon: 0.0695\n",
      "Episode 533 - Total Reward: -380.5762643431874, Moving Average Reward: -251.92, Epsilon: 0.0691\n",
      "Episode 534 - Total Reward: -385.3212683436268, Moving Average Reward: -252.86, Epsilon: 0.0688\n",
      "Episode 535 - Total Reward: -318.8902730743404, Moving Average Reward: -256.33, Epsilon: 0.0684\n",
      "Episode 536 - Total Reward: -248.939660334486, Moving Average Reward: -258.94, Epsilon: 0.0681\n",
      "Episode 537 - Total Reward: -279.61222704966053, Moving Average Reward: -258.79, Epsilon: 0.0678\n",
      "Episode 538 - Total Reward: -267.64616065173936, Moving Average Reward: -260.67, Epsilon: 0.0674\n",
      "Episode 539 - Total Reward: -261.85353970623487, Moving Average Reward: -262.07, Epsilon: 0.0671\n",
      "Episode 540 - Total Reward: -315.2583014015708, Moving Average Reward: -262.81, Epsilon: 0.0668\n",
      "Episode 541 - Total Reward: -41.95555305177653, Moving Average Reward: -261.65, Epsilon: 0.0664\n",
      "Episode 542 - Total Reward: -462.04878810116566, Moving Average Reward: -265.75, Epsilon: 0.0661\n",
      "Episode 543 - Total Reward: -194.53464866751435, Moving Average Reward: -265.81, Epsilon: 0.0658\n",
      "Episode 544 - Total Reward: -407.0961060764977, Moving Average Reward: -267.47, Epsilon: 0.0654\n",
      "Episode 545 - Total Reward: -330.3445232062343, Moving Average Reward: -266.95, Epsilon: 0.0651\n",
      "Episode 546 - Total Reward: -322.0328259272623, Moving Average Reward: -267.42, Epsilon: 0.0648\n",
      "Episode 547 - Total Reward: -208.47731206781165, Moving Average Reward: -268.21, Epsilon: 0.0645\n",
      "Episode 548 - Total Reward: -183.85145244642075, Moving Average Reward: -268.05, Epsilon: 0.0641\n",
      "Episode 549 - Total Reward: -288.25137556183114, Moving Average Reward: -270.47, Epsilon: 0.0638\n",
      "Episode 550 - Total Reward: -235.88597078661948, Moving Average Reward: -270.23, Epsilon: 0.0635\n",
      "Episode 551 - Total Reward: -259.86677262839135, Moving Average Reward: -268.29, Epsilon: 0.0632\n",
      "Episode 552 - Total Reward: -281.23096074008686, Moving Average Reward: -270.71, Epsilon: 0.0629\n",
      "Episode 553 - Total Reward: -193.4959790599827, Moving Average Reward: -271.09, Epsilon: 0.0625\n",
      "Episode 554 - Total Reward: -444.02817248233765, Moving Average Reward: -272.55, Epsilon: 0.0622\n",
      "Episode 555 - Total Reward: -415.1503501890113, Moving Average Reward: -274.27, Epsilon: 0.0619\n",
      "Episode 556 - Total Reward: -356.5934923913036, Moving Average Reward: -276.26, Epsilon: 0.0616\n",
      "Episode 557 - Total Reward: -337.2966044844659, Moving Average Reward: -278.44, Epsilon: 0.0613\n",
      "Episode 558 - Total Reward: -405.88277757507126, Moving Average Reward: -278.27, Epsilon: 0.0610\n",
      "Episode 559 - Total Reward: -430.57273127378744, Moving Average Reward: -280.22, Epsilon: 0.0607\n",
      "Episode 560 - Total Reward: -507.8175772664731, Moving Average Reward: -280.79, Epsilon: 0.0604\n",
      "Episode 561 - Total Reward: -428.61136412980846, Moving Average Reward: -283.08, Epsilon: 0.0601\n",
      "Episode 562 - Total Reward: -230.96950274383624, Moving Average Reward: -283.10, Epsilon: 0.0598\n",
      "Episode 563 - Total Reward: -274.2174821933745, Moving Average Reward: -283.39, Epsilon: 0.0595\n",
      "Episode 564 - Total Reward: -388.61318813001213, Moving Average Reward: -283.00, Epsilon: 0.0592\n",
      "Episode 565 - Total Reward: -403.91855548222566, Moving Average Reward: -287.22, Epsilon: 0.0589\n",
      "Episode 566 - Total Reward: -238.5778136666354, Moving Average Reward: -284.93, Epsilon: 0.0586\n",
      "Episode 567 - Total Reward: -477.34136721463267, Moving Average Reward: -287.50, Epsilon: 0.0583\n",
      "Episode 568 - Total Reward: -282.25598770320676, Moving Average Reward: -288.73, Epsilon: 0.0580\n",
      "Episode 569 - Total Reward: -279.6181289631236, Moving Average Reward: -290.88, Epsilon: 0.0577\n",
      "Episode 570 - Total Reward: -378.60970880523934, Moving Average Reward: -292.79, Epsilon: 0.0574\n",
      "Episode 571 - Total Reward: -343.5370598972944, Moving Average Reward: -296.50, Epsilon: 0.0571\n",
      "Episode 572 - Total Reward: -309.6123953134872, Moving Average Reward: -295.04, Epsilon: 0.0569\n",
      "Episode 573 - Total Reward: -198.8258812602761, Moving Average Reward: -295.47, Epsilon: 0.0566\n",
      "Episode 574 - Total Reward: -195.26338961135698, Moving Average Reward: -294.22, Epsilon: 0.0563\n",
      "Episode 575 - Total Reward: -276.5063139743114, Moving Average Reward: -292.59, Epsilon: 0.0560\n",
      "Episode 576 - Total Reward: -224.9342813615663, Moving Average Reward: -294.84, Epsilon: 0.0557\n",
      "Episode 577 - Total Reward: -241.81436416149143, Moving Average Reward: -294.73, Epsilon: 0.0555\n",
      "Episode 578 - Total Reward: -280.64566879614404, Moving Average Reward: -296.26, Epsilon: 0.0552\n",
      "Episode 579 - Total Reward: -327.551473788385, Moving Average Reward: -298.07, Epsilon: 0.0549\n",
      "Episode 580 - Total Reward: -220.8259708174931, Moving Average Reward: -296.49, Epsilon: 0.0546\n",
      "Episode 581 - Total Reward: -276.5556021669206, Moving Average Reward: -296.98, Epsilon: 0.0544\n",
      "Episode 582 - Total Reward: -328.12454015163803, Moving Average Reward: -297.51, Epsilon: 0.0541\n",
      "Episode 583 - Total Reward: -238.83959435939565, Moving Average Reward: -295.96, Epsilon: 0.0538\n",
      "Episode 584 - Total Reward: -282.8147196437975, Moving Average Reward: -297.13, Epsilon: 0.0535\n",
      "Episode 585 - Total Reward: -394.6782227169424, Moving Average Reward: -298.91, Epsilon: 0.0533\n",
      "Episode 586 - Total Reward: -351.61703138871303, Moving Average Reward: -298.27, Epsilon: 0.0530\n",
      "Episode 587 - Total Reward: -105.85524232675147, Moving Average Reward: -297.56, Epsilon: 0.0527\n",
      "Episode 588 - Total Reward: -271.69681989060325, Moving Average Reward: -298.99, Epsilon: 0.0525\n",
      "Episode 589 - Total Reward: -72.68455946274987, Moving Average Reward: -296.49, Epsilon: 0.0522\n",
      "Episode 590 - Total Reward: -316.2539464383749, Moving Average Reward: -296.96, Epsilon: 0.0520\n",
      "Episode 591 - Total Reward: -425.3875140112081, Moving Average Reward: -296.52, Epsilon: 0.0517\n",
      "Episode 592 - Total Reward: -271.53770771743507, Moving Average Reward: -295.44, Epsilon: 0.0514\n",
      "Episode 593 - Total Reward: -69.8093644965796, Moving Average Reward: -294.72, Epsilon: 0.0512\n",
      "Episode 594 - Total Reward: -316.09611600920636, Moving Average Reward: -293.46, Epsilon: 0.0509\n",
      "Episode 595 - Total Reward: -265.4660471250947, Moving Average Reward: -291.56, Epsilon: 0.0507\n",
      "Episode 596 - Total Reward: -479.0079628932269, Moving Average Reward: -291.79, Epsilon: 0.0504\n",
      "Episode 597 - Total Reward: -109.71966381993265, Moving Average Reward: -288.27, Epsilon: 0.0502\n",
      "Episode 598 - Total Reward: -341.1278125269786, Moving Average Reward: -289.12, Epsilon: 0.0499\n",
      "Episode 599 - Total Reward: -169.63312152096734, Moving Average Reward: -288.38, Epsilon: 0.0497\n",
      "Episode 600 - Total Reward: -414.17270601519544, Moving Average Reward: -291.72, Epsilon: 0.0494\n",
      "Episode 601 - Total Reward: -379.95261433743275, Moving Average Reward: -293.47, Epsilon: 0.0492\n",
      "Episode 602 - Total Reward: -291.42025496501344, Moving Average Reward: -292.69, Epsilon: 0.0489\n",
      "Episode 603 - Total Reward: -232.35505570702247, Moving Average Reward: -291.90, Epsilon: 0.0487\n",
      "Episode 604 - Total Reward: -330.78584514196797, Moving Average Reward: -290.98, Epsilon: 0.0484\n",
      "Episode 605 - Total Reward: -203.59831682654044, Moving Average Reward: -290.42, Epsilon: 0.0482\n",
      "Episode 606 - Total Reward: -335.6204480423846, Moving Average Reward: -291.27, Epsilon: 0.0479\n",
      "Episode 607 - Total Reward: -250.90090380122697, Moving Average Reward: -291.16, Epsilon: 0.0477\n",
      "Episode 608 - Total Reward: -176.26690520029445, Moving Average Reward: -288.02, Epsilon: 0.0475\n",
      "Episode 609 - Total Reward: -399.45167553970697, Moving Average Reward: -288.48, Epsilon: 0.0472\n",
      "Episode 610 - Total Reward: -584.6934253690625, Moving Average Reward: -292.49, Epsilon: 0.0470\n",
      "Episode 611 - Total Reward: -269.4868334509664, Moving Average Reward: -291.60, Epsilon: 0.0468\n",
      "Episode 612 - Total Reward: -247.85664396973343, Moving Average Reward: -292.74, Epsilon: 0.0465\n",
      "Episode 613 - Total Reward: -435.65632464088935, Moving Average Reward: -294.43, Epsilon: 0.0463\n",
      "Episode 614 - Total Reward: -267.3861041341578, Moving Average Reward: -294.72, Epsilon: 0.0461\n",
      "Episode 615 - Total Reward: -139.0836003927193, Moving Average Reward: -292.86, Epsilon: 0.0458\n",
      "Episode 616 - Total Reward: -143.71818813621326, Moving Average Reward: -293.20, Epsilon: 0.0456\n",
      "Episode 617 - Total Reward: -386.10699187425234, Moving Average Reward: -293.69, Epsilon: 0.0454\n",
      "Episode 618 - Total Reward: -247.62931273698706, Moving Average Reward: -294.76, Epsilon: 0.0452\n",
      "Episode 619 - Total Reward: -397.7387972869358, Moving Average Reward: -295.71, Epsilon: 0.0449\n",
      "Episode 620 - Total Reward: -305.9678542231975, Moving Average Reward: -295.70, Epsilon: 0.0447\n",
      "Episode 621 - Total Reward: -285.60424119335, Moving Average Reward: -296.92, Epsilon: 0.0445\n",
      "Episode 622 - Total Reward: -195.52915901503005, Moving Average Reward: -294.74, Epsilon: 0.0443\n",
      "Episode 623 - Total Reward: -439.90883313892135, Moving Average Reward: -295.14, Epsilon: 0.0440\n",
      "Episode 624 - Total Reward: -155.67395919710984, Moving Average Reward: -294.01, Epsilon: 0.0438\n",
      "Episode 625 - Total Reward: -145.4681782315405, Moving Average Reward: -291.78, Epsilon: 0.0436\n",
      "Episode 626 - Total Reward: -237.65935768937123, Moving Average Reward: -291.39, Epsilon: 0.0434\n",
      "Episode 627 - Total Reward: -240.44339680976515, Moving Average Reward: -292.11, Epsilon: 0.0432\n",
      "Episode 628 - Total Reward: -274.1125438570964, Moving Average Reward: -292.15, Epsilon: 0.0429\n",
      "Episode 629 - Total Reward: -336.9462211521206, Moving Average Reward: -294.21, Epsilon: 0.0427\n",
      "Episode 630 - Total Reward: -350.14266373279895, Moving Average Reward: -295.65, Epsilon: 0.0425\n",
      "Episode 631 - Total Reward: -330.27572103647117, Moving Average Reward: -297.15, Epsilon: 0.0423\n",
      "Episode 632 - Total Reward: -220.25971094982947, Moving Average Reward: -295.06, Epsilon: 0.0421\n",
      "Episode 633 - Total Reward: -159.85325714785998, Moving Average Reward: -292.85, Epsilon: 0.0419\n",
      "Episode 634 - Total Reward: -383.4994742908637, Moving Average Reward: -292.83, Epsilon: 0.0417\n",
      "Episode 635 - Total Reward: -264.60166014596655, Moving Average Reward: -292.29, Epsilon: 0.0415\n",
      "Episode 636 - Total Reward: -52.922004440468164, Moving Average Reward: -290.33, Epsilon: 0.0413\n",
      "Episode 637 - Total Reward: -306.1547945876301, Moving Average Reward: -290.59, Epsilon: 0.0410\n",
      "Episode 638 - Total Reward: -281.43876763373873, Moving Average Reward: -290.73, Epsilon: 0.0408\n",
      "Episode 639 - Total Reward: -263.8848409439229, Moving Average Reward: -290.75, Epsilon: 0.0406\n",
      "Episode 640 - Total Reward: -126.83958483835562, Moving Average Reward: -288.87, Epsilon: 0.0404\n",
      "Episode 641 - Total Reward: -123.9327048296324, Moving Average Reward: -289.69, Epsilon: 0.0402\n",
      "Episode 642 - Total Reward: -267.652417060963, Moving Average Reward: -287.74, Epsilon: 0.0400\n",
      "Episode 643 - Total Reward: -102.77740257487432, Moving Average Reward: -286.82, Epsilon: 0.0398\n",
      "Episode 644 - Total Reward: -112.60454687785969, Moving Average Reward: -283.88, Epsilon: 0.0396\n",
      "Episode 645 - Total Reward: -217.0565980273953, Moving Average Reward: -282.75, Epsilon: 0.0394\n",
      "Episode 646 - Total Reward: -243.98254277270766, Moving Average Reward: -281.97, Epsilon: 0.0392\n",
      "Episode 647 - Total Reward: -301.31215144702514, Moving Average Reward: -282.89, Epsilon: 0.0390\n",
      "Episode 648 - Total Reward: -286.4502585123799, Moving Average Reward: -283.92, Epsilon: 0.0388\n",
      "Episode 649 - Total Reward: -396.0243978754597, Moving Average Reward: -285.00, Epsilon: 0.0387\n",
      "Episode 650 - Total Reward: -155.1972125081138, Moving Average Reward: -284.19, Epsilon: 0.0385\n",
      "Episode 651 - Total Reward: -190.69321158018147, Moving Average Reward: -283.50, Epsilon: 0.0383\n",
      "Episode 652 - Total Reward: -163.09284872338264, Moving Average Reward: -282.32, Epsilon: 0.0381\n",
      "Episode 653 - Total Reward: -267.35491542653773, Moving Average Reward: -283.06, Epsilon: 0.0379\n",
      "Episode 654 - Total Reward: -313.60517657620767, Moving Average Reward: -281.75, Epsilon: 0.0377\n",
      "Episode 655 - Total Reward: -360.77518424260757, Moving Average Reward: -281.21, Epsilon: 0.0375\n",
      "Episode 656 - Total Reward: -539.0439515937401, Moving Average Reward: -283.03, Epsilon: 0.0373\n",
      "Episode 657 - Total Reward: -262.6934055049514, Moving Average Reward: -282.29, Epsilon: 0.0371\n",
      "Episode 658 - Total Reward: -261.14246582370674, Moving Average Reward: -280.84, Epsilon: 0.0369\n",
      "Episode 659 - Total Reward: -379.5268963148342, Moving Average Reward: -280.33, Epsilon: 0.0368\n",
      "Episode 660 - Total Reward: -216.36582968832823, Moving Average Reward: -277.42, Epsilon: 0.0366\n",
      "Episode 661 - Total Reward: -471.24991658076283, Moving Average Reward: -277.84, Epsilon: 0.0364\n",
      "Episode 662 - Total Reward: -324.23814708992, Moving Average Reward: -278.77, Epsilon: 0.0362\n",
      "Episode 663 - Total Reward: -239.23461138836834, Moving Average Reward: -278.42, Epsilon: 0.0360\n",
      "Episode 664 - Total Reward: -335.26707897087647, Moving Average Reward: -277.89, Epsilon: 0.0359\n",
      "Episode 665 - Total Reward: -436.6822702679909, Moving Average Reward: -278.22, Epsilon: 0.0357\n",
      "Episode 666 - Total Reward: -446.74754392350826, Moving Average Reward: -280.30, Epsilon: 0.0355\n",
      "Episode 667 - Total Reward: -137.33281950800324, Moving Average Reward: -276.90, Epsilon: 0.0353\n",
      "Episode 668 - Total Reward: -390.484349816617, Moving Average Reward: -277.98, Epsilon: 0.0351\n",
      "Episode 669 - Total Reward: -17.127660508458064, Moving Average Reward: -275.36, Epsilon: 0.0350\n",
      "Episode 670 - Total Reward: -105.93867197865964, Moving Average Reward: -272.63, Epsilon: 0.0348\n",
      "Episode 671 - Total Reward: -253.1108436784949, Moving Average Reward: -271.73, Epsilon: 0.0346\n",
      "Episode 672 - Total Reward: -252.12293434809862, Moving Average Reward: -271.15, Epsilon: 0.0344\n",
      "Episode 673 - Total Reward: -193.9108167115619, Moving Average Reward: -271.10, Epsilon: 0.0343\n",
      "Episode 674 - Total Reward: -308.9389278156833, Moving Average Reward: -272.24, Epsilon: 0.0341\n",
      "Episode 675 - Total Reward: -482.980309149081, Moving Average Reward: -274.30, Epsilon: 0.0339\n",
      "Episode 676 - Total Reward: -50.60603196114738, Moving Average Reward: -272.56, Epsilon: 0.0338\n",
      "Episode 677 - Total Reward: -233.27984889216987, Moving Average Reward: -272.48, Epsilon: 0.0336\n",
      "Episode 678 - Total Reward: -305.5268631193362, Moving Average Reward: -272.72, Epsilon: 0.0334\n",
      "Episode 679 - Total Reward: 49.74628593348396, Moving Average Reward: -268.95, Epsilon: 0.0333\n",
      "Episode 680 - Total Reward: -159.30867526572757, Moving Average Reward: -268.34, Epsilon: 0.0331\n",
      "Episode 681 - Total Reward: -288.05587884957106, Moving Average Reward: -268.45, Epsilon: 0.0329\n",
      "Episode 682 - Total Reward: -223.56893399042062, Moving Average Reward: -267.41, Epsilon: 0.0328\n",
      "Episode 683 - Total Reward: -202.3821022122612, Moving Average Reward: -267.04, Epsilon: 0.0326\n",
      "Episode 684 - Total Reward: -106.11213963290301, Moving Average Reward: -265.27, Epsilon: 0.0324\n",
      "Episode 685 - Total Reward: -95.53965411211657, Moving Average Reward: -262.28, Epsilon: 0.0323\n",
      "Episode 686 - Total Reward: -216.84679496134248, Moving Average Reward: -260.93, Epsilon: 0.0321\n",
      "Episode 687 - Total Reward: -275.1315244267538, Moving Average Reward: -262.63, Epsilon: 0.0319\n",
      "Episode 688 - Total Reward: -315.553229664564, Moving Average Reward: -263.07, Epsilon: 0.0318\n",
      "Episode 689 - Total Reward: -300.6121697892727, Moving Average Reward: -265.35, Epsilon: 0.0316\n",
      "Episode 690 - Total Reward: -186.37726834686183, Moving Average Reward: -264.05, Epsilon: 0.0315\n",
      "Episode 691 - Total Reward: -284.07631038548686, Moving Average Reward: -262.63, Epsilon: 0.0313\n",
      "Episode 692 - Total Reward: -193.04815156025953, Moving Average Reward: -261.85, Epsilon: 0.0312\n",
      "Episode 693 - Total Reward: -271.0046993662048, Moving Average Reward: -263.86, Epsilon: 0.0310\n",
      "Episode 694 - Total Reward: -118.1045865107038, Moving Average Reward: -261.88, Epsilon: 0.0308\n",
      "Episode 695 - Total Reward: -362.8651229228935, Moving Average Reward: -262.85, Epsilon: 0.0307\n",
      "Episode 696 - Total Reward: -257.5537038478394, Moving Average Reward: -260.64, Epsilon: 0.0305\n",
      "Episode 697 - Total Reward: -293.264598833267, Moving Average Reward: -262.48, Epsilon: 0.0304\n",
      "Episode 698 - Total Reward: -279.5784520470486, Moving Average Reward: -261.86, Epsilon: 0.0302\n",
      "Episode 699 - Total Reward: -138.674627088283, Moving Average Reward: -261.55, Epsilon: 0.0301\n",
      "Episode 700 - Total Reward: -90.4701651482099, Moving Average Reward: -258.31, Epsilon: 0.0299\n",
      "Episode 701 - Total Reward: -7.774511147498956, Moving Average Reward: -254.59, Epsilon: 0.0298\n",
      "Episode 702 - Total Reward: -334.85660568251205, Moving Average Reward: -255.03, Epsilon: 0.0296\n",
      "Episode 703 - Total Reward: -231.2676286060039, Moving Average Reward: -255.02, Epsilon: 0.0295\n",
      "Episode 704 - Total Reward: -181.8922233674244, Moving Average Reward: -253.53, Epsilon: 0.0293\n",
      "Episode 705 - Total Reward: -72.84362396609342, Moving Average Reward: -252.22, Epsilon: 0.0292\n",
      "Episode 706 - Total Reward: -208.4808156335615, Moving Average Reward: -250.95, Epsilon: 0.0290\n",
      "Episode 707 - Total Reward: -250.3835291847779, Moving Average Reward: -250.94, Epsilon: 0.0289\n",
      "Episode 708 - Total Reward: -310.41641145617655, Moving Average Reward: -252.28, Epsilon: 0.0288\n",
      "Episode 709 - Total Reward: -315.8371824820815, Moving Average Reward: -251.45, Epsilon: 0.0286\n",
      "Episode 710 - Total Reward: -216.85341377341922, Moving Average Reward: -247.77, Epsilon: 0.0285\n",
      "Episode 711 - Total Reward: -281.06861481416206, Moving Average Reward: -247.88, Epsilon: 0.0283\n",
      "Episode 712 - Total Reward: -155.7340066385177, Moving Average Reward: -246.96, Epsilon: 0.0282\n",
      "Episode 713 - Total Reward: -63.080019414081285, Moving Average Reward: -243.24, Epsilon: 0.0280\n",
      "Episode 714 - Total Reward: -280.39498147737106, Moving Average Reward: -243.37, Epsilon: 0.0279\n",
      "Episode 715 - Total Reward: -168.49950213718125, Moving Average Reward: -243.66, Epsilon: 0.0278\n",
      "Episode 716 - Total Reward: -291.8552981092464, Moving Average Reward: -245.14, Epsilon: 0.0276\n",
      "Episode 717 - Total Reward: -444.0094746949412, Moving Average Reward: -245.72, Epsilon: 0.0275\n",
      "Episode 718 - Total Reward: -241.9815895269626, Moving Average Reward: -245.67, Epsilon: 0.0274\n",
      "Episode 719 - Total Reward: -86.27351290118081, Moving Average Reward: -242.55, Epsilon: 0.0272\n",
      "Episode 720 - Total Reward: -261.2809638224711, Moving Average Reward: -242.10, Epsilon: 0.0271\n",
      "Episode 721 - Total Reward: -302.6884059130297, Moving Average Reward: -242.28, Epsilon: 0.0269\n",
      "Episode 722 - Total Reward: -133.41946402559955, Moving Average Reward: -241.65, Epsilon: 0.0268\n",
      "Episode 723 - Total Reward: -261.19885961839657, Moving Average Reward: -239.87, Epsilon: 0.0267\n",
      "Episode 724 - Total Reward: -322.2717956189465, Moving Average Reward: -241.53, Epsilon: 0.0265\n",
      "Episode 725 - Total Reward: -268.4400071104719, Moving Average Reward: -242.76, Epsilon: 0.0264\n",
      "Episode 726 - Total Reward: -158.1561919390483, Moving Average Reward: -241.97, Epsilon: 0.0263\n",
      "Episode 727 - Total Reward: -314.8276104144155, Moving Average Reward: -242.71, Epsilon: 0.0261\n",
      "Episode 728 - Total Reward: -367.6974912737713, Moving Average Reward: -243.65, Epsilon: 0.0260\n",
      "Episode 729 - Total Reward: -375.40785348287477, Moving Average Reward: -244.03, Epsilon: 0.0259\n",
      "Episode 730 - Total Reward: -353.0482803058625, Moving Average Reward: -244.06, Epsilon: 0.0258\n",
      "Episode 731 - Total Reward: -443.05847517179734, Moving Average Reward: -245.19, Epsilon: 0.0256\n",
      "Episode 732 - Total Reward: -311.49730592746255, Moving Average Reward: -246.10, Epsilon: 0.0255\n",
      "Episode 733 - Total Reward: -152.98867562140845, Moving Average Reward: -246.03, Epsilon: 0.0254\n",
      "Episode 734 - Total Reward: -222.8074679829592, Moving Average Reward: -244.43, Epsilon: 0.0252\n",
      "Episode 735 - Total Reward: -179.59296740365863, Moving Average Reward: -243.58, Epsilon: 0.0251\n",
      "Episode 736 - Total Reward: -243.5138340630511, Moving Average Reward: -245.48, Epsilon: 0.0250\n",
      "Episode 737 - Total Reward: -380.7162611245522, Moving Average Reward: -246.23, Epsilon: 0.0249\n",
      "Episode 738 - Total Reward: -288.90274870310986, Moving Average Reward: -246.30, Epsilon: 0.0247\n",
      "Episode 739 - Total Reward: -399.17138123453384, Moving Average Reward: -247.65, Epsilon: 0.0246\n",
      "Episode 740 - Total Reward: -380.56787281485873, Moving Average Reward: -250.19, Epsilon: 0.0245\n",
      "Episode 741 - Total Reward: -335.25357430973236, Moving Average Reward: -252.31, Epsilon: 0.0244\n",
      "Episode 742 - Total Reward: -394.3566785949623, Moving Average Reward: -253.57, Epsilon: 0.0243\n",
      "Episode 743 - Total Reward: -285.41190052673596, Moving Average Reward: -255.40, Epsilon: 0.0241\n",
      "Episode 744 - Total Reward: -306.30613778275574, Moving Average Reward: -257.34, Epsilon: 0.0240\n",
      "Episode 745 - Total Reward: -61.2147358095563, Moving Average Reward: -255.78, Epsilon: 0.0239\n",
      "Episode 746 - Total Reward: -173.694022513726, Moving Average Reward: -255.07, Epsilon: 0.0238\n",
      "Episode 747 - Total Reward: -283.3690972185791, Moving Average Reward: -254.89, Epsilon: 0.0237\n",
      "Episode 748 - Total Reward: -327.2262222543561, Moving Average Reward: -255.30, Epsilon: 0.0235\n",
      "Episode 749 - Total Reward: -275.685195751063, Moving Average Reward: -254.10, Epsilon: 0.0234\n",
      "Episode 750 - Total Reward: -287.3405118594055, Moving Average Reward: -255.42, Epsilon: 0.0233\n",
      "Episode 751 - Total Reward: -89.4596640330875, Moving Average Reward: -254.41, Epsilon: 0.0232\n",
      "Episode 752 - Total Reward: -527.1377238131781, Moving Average Reward: -258.05, Epsilon: 0.0231\n",
      "Episode 753 - Total Reward: -344.88376541839466, Moving Average Reward: -258.82, Epsilon: 0.0229\n",
      "Episode 754 - Total Reward: -234.07673795387313, Moving Average Reward: -258.03, Epsilon: 0.0228\n",
      "Episode 755 - Total Reward: -319.59780620015965, Moving Average Reward: -257.62, Epsilon: 0.0227\n",
      "Episode 756 - Total Reward: -96.19816479278596, Moving Average Reward: -253.19, Epsilon: 0.0226\n",
      "Episode 757 - Total Reward: -322.30386734851476, Moving Average Reward: -253.78, Epsilon: 0.0225\n",
      "Episode 758 - Total Reward: -315.39309450630753, Moving Average Reward: -254.33, Epsilon: 0.0224\n",
      "Episode 759 - Total Reward: -221.21110401562794, Moving Average Reward: -252.74, Epsilon: 0.0223\n",
      "Episode 760 - Total Reward: -103.87088373970717, Moving Average Reward: -251.62, Epsilon: 0.0222\n",
      "Episode 761 - Total Reward: -150.8805162262465, Moving Average Reward: -248.42, Epsilon: 0.0220\n",
      "Episode 762 - Total Reward: -105.16365654086522, Moving Average Reward: -246.22, Epsilon: 0.0219\n",
      "Episode 763 - Total Reward: -280.9769453620603, Moving Average Reward: -246.64, Epsilon: 0.0218\n",
      "Episode 764 - Total Reward: -286.77270915071676, Moving Average Reward: -246.16, Epsilon: 0.0217\n",
      "Episode 765 - Total Reward: -260.6080146355235, Moving Average Reward: -244.40, Epsilon: 0.0216\n",
      "Episode 766 - Total Reward: -283.7511643388567, Moving Average Reward: -242.77, Epsilon: 0.0215\n",
      "Episode 767 - Total Reward: -263.98026852713184, Moving Average Reward: -244.03, Epsilon: 0.0214\n",
      "Episode 768 - Total Reward: -141.457600208526, Moving Average Reward: -241.54, Epsilon: 0.0213\n",
      "Episode 769 - Total Reward: -368.7427589452001, Moving Average Reward: -245.06, Epsilon: 0.0212\n",
      "Episode 770 - Total Reward: -285.5841758555883, Moving Average Reward: -246.86, Epsilon: 0.0211\n",
      "Episode 771 - Total Reward: -339.2090476407416, Moving Average Reward: -247.72, Epsilon: 0.0210\n",
      "Episode 772 - Total Reward: -267.6365194881245, Moving Average Reward: -247.87, Epsilon: 0.0209\n",
      "Episode 773 - Total Reward: -372.5792680359008, Moving Average Reward: -249.66, Epsilon: 0.0208\n",
      "Episode 774 - Total Reward: -234.99157086143566, Moving Average Reward: -248.92, Epsilon: 0.0207\n",
      "Episode 775 - Total Reward: -231.40386193967635, Moving Average Reward: -246.40, Epsilon: 0.0206\n",
      "Episode 776 - Total Reward: -68.9759237831673, Moving Average Reward: -246.59, Epsilon: 0.0205\n",
      "Episode 777 - Total Reward: -93.27660676828827, Moving Average Reward: -245.19, Epsilon: 0.0203\n",
      "Episode 778 - Total Reward: -68.07083603401964, Moving Average Reward: -242.81, Epsilon: 0.0202\n",
      "Episode 779 - Total Reward: -247.83299190451177, Moving Average Reward: -245.79, Epsilon: 0.0201\n",
      "Episode 780 - Total Reward: -247.1941871046473, Moving Average Reward: -246.67, Epsilon: 0.0200\n",
      "Episode 781 - Total Reward: -174.4477842047758, Moving Average Reward: -245.53, Epsilon: 0.0199\n",
      "Episode 782 - Total Reward: -362.90191773674667, Moving Average Reward: -246.92, Epsilon: 0.0198\n",
      "Episode 783 - Total Reward: -107.34926894120892, Moving Average Reward: -245.97, Epsilon: 0.0197\n",
      "Episode 784 - Total Reward: -145.82755208669406, Moving Average Reward: -246.37, Epsilon: 0.0196\n",
      "Episode 785 - Total Reward: -223.51312594207317, Moving Average Reward: -247.65, Epsilon: 0.0195\n",
      "Episode 786 - Total Reward: 42.71280398363055, Moving Average Reward: -245.05, Epsilon: 0.0195\n",
      "Episode 787 - Total Reward: -82.5001268718801, Moving Average Reward: -243.13, Epsilon: 0.0194\n",
      "Episode 788 - Total Reward: -235.32952357849808, Moving Average Reward: -242.33, Epsilon: 0.0193\n",
      "Episode 789 - Total Reward: -278.76321492325854, Moving Average Reward: -242.11, Epsilon: 0.0192\n",
      "Episode 790 - Total Reward: -130.58479337899684, Moving Average Reward: -241.55, Epsilon: 0.0191\n",
      "Episode 791 - Total Reward: -63.39619893355846, Moving Average Reward: -239.34, Epsilon: 0.0190\n",
      "Episode 792 - Total Reward: -345.10497207966205, Moving Average Reward: -240.86, Epsilon: 0.0189\n",
      "Episode 793 - Total Reward: -287.7319297338772, Moving Average Reward: -241.03, Epsilon: 0.0188\n",
      "Episode 794 - Total Reward: -285.59447556003374, Moving Average Reward: -242.71, Epsilon: 0.0187\n",
      "Episode 795 - Total Reward: -247.28580295903774, Moving Average Reward: -241.55, Epsilon: 0.0186\n",
      "Episode 796 - Total Reward: -268.7099716173628, Moving Average Reward: -241.66, Epsilon: 0.0185\n",
      "Episode 797 - Total Reward: -216.8921654614021, Moving Average Reward: -240.90, Epsilon: 0.0184\n",
      "Episode 798 - Total Reward: -274.8640643902884, Moving Average Reward: -240.85, Epsilon: 0.0183\n",
      "Episode 799 - Total Reward: -236.23397211191846, Moving Average Reward: -241.83, Epsilon: 0.0182\n",
      "Episode 800 - Total Reward: -402.20936029765556, Moving Average Reward: -244.94, Epsilon: 0.0181\n",
      "Episode 801 - Total Reward: -179.5645519109556, Moving Average Reward: -246.66, Epsilon: 0.0180\n",
      "Episode 802 - Total Reward: -385.3555756744532, Moving Average Reward: -247.17, Epsilon: 0.0180\n",
      "Episode 803 - Total Reward: -312.8827292342149, Moving Average Reward: -247.98, Epsilon: 0.0179\n",
      "Episode 804 - Total Reward: -284.5491538392679, Moving Average Reward: -249.01, Epsilon: 0.0178\n",
      "Episode 805 - Total Reward: -270.5671490767909, Moving Average Reward: -250.99, Epsilon: 0.0177\n",
      "Episode 806 - Total Reward: -301.95011190215644, Moving Average Reward: -251.92, Epsilon: 0.0176\n",
      "Episode 807 - Total Reward: -256.1168740802624, Moving Average Reward: -251.98, Epsilon: 0.0175\n",
      "Episode 808 - Total Reward: -385.53237061105284, Moving Average Reward: -252.73, Epsilon: 0.0174\n",
      "Episode 809 - Total Reward: -62.51339600212952, Moving Average Reward: -250.20, Epsilon: 0.0173\n",
      "Episode 810 - Total Reward: -308.7730682714713, Moving Average Reward: -251.12, Epsilon: 0.0172\n",
      "Episode 811 - Total Reward: -18.780348241913117, Moving Average Reward: -248.49, Epsilon: 0.0172\n",
      "Episode 812 - Total Reward: -70.90867334376637, Moving Average Reward: -247.64, Epsilon: 0.0171\n",
      "Episode 813 - Total Reward: -55.311022675268916, Moving Average Reward: -247.57, Epsilon: 0.0170\n",
      "Episode 814 - Total Reward: -171.04323757392888, Moving Average Reward: -246.47, Epsilon: 0.0169\n",
      "Episode 815 - Total Reward: -311.76834268108126, Moving Average Reward: -247.91, Epsilon: 0.0168\n",
      "Episode 816 - Total Reward: -80.77460842241764, Moving Average Reward: -245.80, Epsilon: 0.0167\n",
      "Episode 817 - Total Reward: -122.43155083473124, Moving Average Reward: -242.58, Epsilon: 0.0167\n",
      "Episode 818 - Total Reward: -122.78517620319943, Moving Average Reward: -241.39, Epsilon: 0.0166\n",
      "Episode 819 - Total Reward: -285.4566870230926, Moving Average Reward: -243.38, Epsilon: 0.0165\n",
      "Episode 820 - Total Reward: -257.26275464543534, Moving Average Reward: -243.34, Epsilon: 0.0164\n",
      "Episode 821 - Total Reward: -249.09267730683032, Moving Average Reward: -242.80, Epsilon: 0.0163\n",
      "Episode 822 - Total Reward: -275.36706975609457, Moving Average Reward: -244.22, Epsilon: 0.0162\n",
      "Episode 823 - Total Reward: -365.96847197533856, Moving Average Reward: -245.27, Epsilon: 0.0162\n",
      "Episode 824 - Total Reward: -134.60722475574698, Moving Average Reward: -243.39, Epsilon: 0.0161\n",
      "Episode 825 - Total Reward: -284.6807978967977, Moving Average Reward: -243.56, Epsilon: 0.0160\n",
      "Episode 826 - Total Reward: -73.0645468863736, Moving Average Reward: -242.71, Epsilon: 0.0159\n",
      "Episode 827 - Total Reward: -320.5126025782979, Moving Average Reward: -242.76, Epsilon: 0.0158\n",
      "Episode 828 - Total Reward: -85.56880175905404, Moving Average Reward: -239.94, Epsilon: 0.0158\n",
      "Episode 829 - Total Reward: -107.11200532264579, Moving Average Reward: -237.26, Epsilon: 0.0157\n",
      "Episode 830 - Total Reward: -377.09775750919005, Moving Average Reward: -237.50, Epsilon: 0.0156\n",
      "Episode 831 - Total Reward: -282.61799297152993, Moving Average Reward: -235.89, Epsilon: 0.0155\n",
      "Episode 832 - Total Reward: -264.3474413443298, Moving Average Reward: -235.42, Epsilon: 0.0154\n",
      "Episode 833 - Total Reward: -103.06354044342206, Moving Average Reward: -234.92, Epsilon: 0.0154\n",
      "Episode 834 - Total Reward: -379.17742604955765, Moving Average Reward: -236.49, Epsilon: 0.0153\n",
      "Episode 835 - Total Reward: -315.10452957322616, Moving Average Reward: -237.84, Epsilon: 0.0152\n",
      "Episode 836 - Total Reward: -275.975073197591, Moving Average Reward: -238.17, Epsilon: 0.0151\n",
      "Episode 837 - Total Reward: -75.41413450072156, Moving Average Reward: -235.11, Epsilon: 0.0151\n",
      "Episode 838 - Total Reward: -172.3069823299724, Moving Average Reward: -233.95, Epsilon: 0.0150\n",
      "Episode 839 - Total Reward: -354.1284491210454, Moving Average Reward: -233.50, Epsilon: 0.0149\n",
      "Episode 840 - Total Reward: -193.19049978361335, Moving Average Reward: -231.62, Epsilon: 0.0148\n",
      "Episode 841 - Total Reward: -355.11261278833933, Moving Average Reward: -231.82, Epsilon: 0.0148\n",
      "Episode 842 - Total Reward: -228.7082478949865, Moving Average Reward: -230.17, Epsilon: 0.0147\n",
      "Episode 843 - Total Reward: -387.87646291173024, Moving Average Reward: -231.19, Epsilon: 0.0146\n",
      "Episode 844 - Total Reward: -65.9299757059308, Moving Average Reward: -228.79, Epsilon: 0.0145\n",
      "Episode 845 - Total Reward: -334.48971156560435, Moving Average Reward: -231.52, Epsilon: 0.0145\n",
      "Episode 846 - Total Reward: -233.32630645955024, Moving Average Reward: -232.12, Epsilon: 0.0144\n",
      "Episode 847 - Total Reward: -121.90944655958823, Moving Average Reward: -230.50, Epsilon: 0.0143\n",
      "Episode 848 - Total Reward: -275.8165753388513, Moving Average Reward: -229.99, Epsilon: 0.0143\n",
      "Episode 849 - Total Reward: -287.9347677433193, Moving Average Reward: -230.11, Epsilon: 0.0142\n",
      "Episode 850 - Total Reward: -106.94151854838084, Moving Average Reward: -228.31, Epsilon: 0.0141\n",
      "Episode 851 - Total Reward: -255.93452235187152, Moving Average Reward: -229.97, Epsilon: 0.0140\n",
      "Episode 852 - Total Reward: -265.838988427859, Moving Average Reward: -227.36, Epsilon: 0.0140\n",
      "Episode 853 - Total Reward: -297.7257716552737, Moving Average Reward: -226.89, Epsilon: 0.0139\n",
      "Episode 854 - Total Reward: -293.51921395111066, Moving Average Reward: -227.48, Epsilon: 0.0138\n",
      "Episode 855 - Total Reward: -278.818330272276, Moving Average Reward: -227.07, Epsilon: 0.0138\n",
      "Episode 856 - Total Reward: -286.3892906536631, Moving Average Reward: -228.97, Epsilon: 0.0137\n",
      "Episode 857 - Total Reward: -238.44227552674243, Moving Average Reward: -228.14, Epsilon: 0.0136\n",
      "Episode 858 - Total Reward: -64.99861770960308, Moving Average Reward: -225.63, Epsilon: 0.0136\n",
      "Episode 859 - Total Reward: -245.2606917986725, Moving Average Reward: -225.87, Epsilon: 0.0135\n",
      "Episode 860 - Total Reward: -257.32398862157845, Moving Average Reward: -227.41, Epsilon: 0.0134\n",
      "Episode 861 - Total Reward: -280.5238836546795, Moving Average Reward: -228.70, Epsilon: 0.0134\n",
      "Episode 862 - Total Reward: -234.41044166965608, Moving Average Reward: -230.00, Epsilon: 0.0133\n",
      "Episode 863 - Total Reward: -196.85304317590922, Moving Average Reward: -229.15, Epsilon: 0.0132\n",
      "Episode 864 - Total Reward: -307.3346134663122, Moving Average Reward: -229.36, Epsilon: 0.0132\n",
      "Episode 865 - Total Reward: -314.0358770911296, Moving Average Reward: -229.89, Epsilon: 0.0131\n",
      "Episode 866 - Total Reward: -341.99759411941125, Moving Average Reward: -230.48, Epsilon: 0.0130\n",
      "Episode 867 - Total Reward: -198.40365713837758, Moving Average Reward: -229.82, Epsilon: 0.0130\n",
      "Episode 868 - Total Reward: -271.3684085210847, Moving Average Reward: -231.12, Epsilon: 0.0129\n",
      "Episode 869 - Total Reward: -339.900474427106, Moving Average Reward: -230.83, Epsilon: 0.0128\n",
      "Episode 870 - Total Reward: -257.4131668285489, Moving Average Reward: -230.55, Epsilon: 0.0128\n",
      "Episode 871 - Total Reward: -309.1834124732102, Moving Average Reward: -230.25, Epsilon: 0.0127\n",
      "Episode 872 - Total Reward: -126.24882051157003, Moving Average Reward: -228.84, Epsilon: 0.0126\n",
      "Episode 873 - Total Reward: -262.71797837454505, Moving Average Reward: -227.74, Epsilon: 0.0126\n",
      "Episode 874 - Total Reward: -87.21356828367207, Moving Average Reward: -226.26, Epsilon: 0.0125\n",
      "Episode 875 - Total Reward: -104.30946223481875, Moving Average Reward: -224.99, Epsilon: 0.0125\n",
      "Episode 876 - Total Reward: -236.22285239330802, Moving Average Reward: -226.66, Epsilon: 0.0124\n",
      "Episode 877 - Total Reward: -196.18042211965144, Moving Average Reward: -227.69, Epsilon: 0.0123\n",
      "Episode 878 - Total Reward: -223.85310022889553, Moving Average Reward: -229.25, Epsilon: 0.0123\n",
      "Episode 879 - Total Reward: -356.1714148677921, Moving Average Reward: -230.33, Epsilon: 0.0122\n",
      "Episode 880 - Total Reward: -256.2146541463069, Moving Average Reward: -230.42, Epsilon: 0.0121\n",
      "Episode 881 - Total Reward: -105.17414704773928, Moving Average Reward: -229.73, Epsilon: 0.0121\n",
      "Episode 882 - Total Reward: -268.0540885782257, Moving Average Reward: -228.78, Epsilon: 0.0120\n",
      "Episode 883 - Total Reward: -227.68181288900882, Moving Average Reward: -229.98, Epsilon: 0.0120\n",
      "Episode 884 - Total Reward: -216.3940697973544, Moving Average Reward: -230.69, Epsilon: 0.0119\n",
      "Episode 885 - Total Reward: -316.61485712793285, Moving Average Reward: -231.62, Epsilon: 0.0118\n",
      "Episode 886 - Total Reward: -258.4665921720133, Moving Average Reward: -234.63, Epsilon: 0.0118\n",
      "Episode 887 - Total Reward: -312.7572522421639, Moving Average Reward: -236.93, Epsilon: 0.0117\n",
      "Episode 888 - Total Reward: -319.27642306540235, Moving Average Reward: -237.77, Epsilon: 0.0117\n",
      "Episode 889 - Total Reward: -291.7589104982162, Moving Average Reward: -237.90, Epsilon: 0.0116\n",
      "Episode 890 - Total Reward: -288.0311732137794, Moving Average Reward: -239.48, Epsilon: 0.0115\n",
      "Episode 891 - Total Reward: -154.9903468983169, Moving Average Reward: -240.39, Epsilon: 0.0115\n",
      "Episode 892 - Total Reward: -275.1182828599981, Moving Average Reward: -239.69, Epsilon: 0.0114\n",
      "Episode 893 - Total Reward: -40.644253595150175, Moving Average Reward: -237.22, Epsilon: 0.0114\n",
      "Episode 894 - Total Reward: -296.5175892275512, Moving Average Reward: -237.33, Epsilon: 0.0113\n",
      "Episode 895 - Total Reward: -78.92533842762583, Moving Average Reward: -235.65, Epsilon: 0.0113\n",
      "Episode 896 - Total Reward: -53.30887082983975, Moving Average Reward: -233.49, Epsilon: 0.0112\n",
      "Episode 897 - Total Reward: -44.52387595201956, Moving Average Reward: -231.77, Epsilon: 0.0112\n",
      "Episode 898 - Total Reward: -76.55120036133948, Moving Average Reward: -229.79, Epsilon: 0.0111\n",
      "Episode 899 - Total Reward: -267.5645020747524, Moving Average Reward: -230.10, Epsilon: 0.0110\n",
      "Episode 900 - Total Reward: -258.3661448802893, Moving Average Reward: -228.66, Epsilon: 0.0110\n",
      "Episode 901 - Total Reward: -209.6080602882826, Moving Average Reward: -228.96, Epsilon: 0.0109\n",
      "Episode 902 - Total Reward: -514.6291016196471, Moving Average Reward: -230.26, Epsilon: 0.0109\n",
      "Episode 903 - Total Reward: -291.3085490155403, Moving Average Reward: -230.04, Epsilon: 0.0108\n",
      "Episode 904 - Total Reward: -382.9732544986524, Moving Average Reward: -231.02, Epsilon: 0.0108\n",
      "Episode 905 - Total Reward: -235.84146896480698, Moving Average Reward: -230.68, Epsilon: 0.0107\n",
      "Episode 906 - Total Reward: -227.24011356679657, Moving Average Reward: -229.93, Epsilon: 0.0107\n",
      "Episode 907 - Total Reward: -160.17817814931846, Moving Average Reward: -228.97, Epsilon: 0.0106\n",
      "Episode 908 - Total Reward: -260.8395353265114, Moving Average Reward: -227.72, Epsilon: 0.0106\n",
      "Episode 909 - Total Reward: -259.09948206079764, Moving Average Reward: -229.69, Epsilon: 0.0105\n",
      "Episode 910 - Total Reward: -275.20988229248945, Moving Average Reward: -229.35, Epsilon: 0.0104\n",
      "Episode 911 - Total Reward: -60.57166591772233, Moving Average Reward: -229.77, Epsilon: 0.0104\n",
      "Episode 912 - Total Reward: -146.30706783724926, Moving Average Reward: -230.53, Epsilon: 0.0103\n",
      "Episode 913 - Total Reward: -225.78918563190405, Moving Average Reward: -232.23, Epsilon: 0.0103\n",
      "Episode 914 - Total Reward: -282.68200315426566, Moving Average Reward: -233.35, Epsilon: 0.0102\n",
      "Episode 915 - Total Reward: -315.80579629914865, Moving Average Reward: -233.39, Epsilon: 0.0102\n",
      "Episode 916 - Total Reward: -260.0224584532454, Moving Average Reward: -235.18, Epsilon: 0.0101\n",
      "Episode 917 - Total Reward: -244.87264947651968, Moving Average Reward: -236.40, Epsilon: 0.0101\n",
      "Episode 918 - Total Reward: -258.03156919232424, Moving Average Reward: -237.76, Epsilon: 0.0100\n",
      "Episode 919 - Total Reward: -787.3362922699363, Moving Average Reward: -242.78, Epsilon: 0.0100\n",
      "Episode 920 - Total Reward: -247.45390612391463, Moving Average Reward: -242.68, Epsilon: 0.0100\n",
      "Episode 921 - Total Reward: -275.21537498616306, Moving Average Reward: -242.94, Epsilon: 0.0100\n",
      "Episode 922 - Total Reward: -99.77775611577877, Moving Average Reward: -241.18, Epsilon: 0.0100\n",
      "Episode 923 - Total Reward: 11.991153791576735, Moving Average Reward: -237.40, Epsilon: 0.0100\n",
      "Episode 924 - Total Reward: -257.9061130533687, Moving Average Reward: -238.64, Epsilon: 0.0100\n",
      "Episode 925 - Total Reward: -75.1052310500768, Moving Average Reward: -236.54, Epsilon: 0.0100\n",
      "Episode 926 - Total Reward: 18.56270292742559, Moving Average Reward: -235.62, Epsilon: 0.0100\n",
      "Episode 927 - Total Reward: -283.59566868065554, Moving Average Reward: -235.26, Epsilon: 0.0100\n",
      "Episode 928 - Total Reward: -186.93113387530917, Moving Average Reward: -236.27, Epsilon: 0.0100\n",
      "Episode 929 - Total Reward: -254.1860439692751, Moving Average Reward: -237.74, Epsilon: 0.0100\n",
      "Episode 930 - Total Reward: -238.79379288163918, Moving Average Reward: -236.36, Epsilon: 0.0100\n",
      "Episode 931 - Total Reward: -262.22897553631907, Moving Average Reward: -236.15, Epsilon: 0.0100\n",
      "Episode 932 - Total Reward: -252.53627652463928, Moving Average Reward: -236.03, Epsilon: 0.0100\n",
      "Episode 933 - Total Reward: -209.82888564607703, Moving Average Reward: -237.10, Epsilon: 0.0100\n",
      "Episode 934 - Total Reward: -238.10741759847457, Moving Average Reward: -235.69, Epsilon: 0.0100\n",
      "Episode 935 - Total Reward: -376.756861864074, Moving Average Reward: -236.31, Epsilon: 0.0100\n",
      "Episode 936 - Total Reward: -245.3562467010699, Moving Average Reward: -236.00, Epsilon: 0.0100\n",
      "Episode 937 - Total Reward: -248.36602476445614, Moving Average Reward: -237.73, Epsilon: 0.0100\n",
      "Episode 938 - Total Reward: -222.26422313489473, Moving Average Reward: -238.23, Epsilon: 0.0100\n",
      "Episode 939 - Total Reward: -258.2204099194553, Moving Average Reward: -237.27, Epsilon: 0.0100\n",
      "Episode 940 - Total Reward: -259.40838715257064, Moving Average Reward: -237.93, Epsilon: 0.0100\n",
      "Episode 941 - Total Reward: -254.74208239885536, Moving Average Reward: -236.93, Epsilon: 0.0100\n",
      "Episode 942 - Total Reward: -477.61507307125396, Moving Average Reward: -239.42, Epsilon: 0.0100\n",
      "Episode 943 - Total Reward: -211.19693256789913, Moving Average Reward: -237.65, Epsilon: 0.0100\n",
      "Episode 944 - Total Reward: -233.68807986458665, Moving Average Reward: -239.33, Epsilon: 0.0100\n",
      "Episode 945 - Total Reward: -302.7081708192612, Moving Average Reward: -239.01, Epsilon: 0.0100\n",
      "Episode 946 - Total Reward: -224.36803790908795, Moving Average Reward: -238.92, Epsilon: 0.0100\n",
      "Episode 947 - Total Reward: -252.9853588012443, Moving Average Reward: -240.23, Epsilon: 0.0100\n",
      "Episode 948 - Total Reward: -224.57905286904577, Moving Average Reward: -239.72, Epsilon: 0.0100\n",
      "Episode 949 - Total Reward: -207.01515203885413, Moving Average Reward: -238.91, Epsilon: 0.0100\n",
      "Episode 950 - Total Reward: -284.4747659897195, Moving Average Reward: -240.69, Epsilon: 0.0100\n",
      "Episode 951 - Total Reward: -242.47643873713434, Moving Average Reward: -240.55, Epsilon: 0.0100\n",
      "Episode 952 - Total Reward: -246.17250774791233, Moving Average Reward: -240.36, Epsilon: 0.0100\n",
      "Episode 953 - Total Reward: -251.35436076509396, Moving Average Reward: -239.89, Epsilon: 0.0100\n",
      "Episode 954 - Total Reward: -274.530662987557, Moving Average Reward: -239.70, Epsilon: 0.0100\n",
      "Episode 955 - Total Reward: -227.57304050231846, Moving Average Reward: -239.19, Epsilon: 0.0100\n",
      "Episode 956 - Total Reward: -263.21617381945896, Moving Average Reward: -238.96, Epsilon: 0.0100\n",
      "Episode 957 - Total Reward: -237.571163255503, Moving Average Reward: -238.95, Epsilon: 0.0100\n",
      "Episode 958 - Total Reward: -450.52342515155243, Moving Average Reward: -242.80, Epsilon: 0.0100\n",
      "Episode 959 - Total Reward: -203.0207864669514, Moving Average Reward: -242.38, Epsilon: 0.0100\n",
      "Episode 960 - Total Reward: -198.05127545223766, Moving Average Reward: -241.79, Epsilon: 0.0100\n",
      "Episode 961 - Total Reward: -261.72717189018516, Moving Average Reward: -241.60, Epsilon: 0.0100\n",
      "Episode 962 - Total Reward: -184.36880647739224, Moving Average Reward: -241.10, Epsilon: 0.0100\n",
      "Episode 963 - Total Reward: -250.9635366767256, Moving Average Reward: -241.64, Epsilon: 0.0100\n",
      "Episode 964 - Total Reward: -222.05151831905675, Moving Average Reward: -240.79, Epsilon: 0.0100\n",
      "Episode 965 - Total Reward: -290.33864375228154, Moving Average Reward: -240.55, Epsilon: 0.0100\n",
      "Episode 966 - Total Reward: -286.44961020907385, Moving Average Reward: -240.00, Epsilon: 0.0100\n",
      "Episode 967 - Total Reward: -260.0874473446961, Moving Average Reward: -240.61, Epsilon: 0.0100\n",
      "Episode 968 - Total Reward: -242.19406928152253, Moving Average Reward: -240.32, Epsilon: 0.0100\n",
      "Episode 969 - Total Reward: -244.38270075145073, Moving Average Reward: -239.37, Epsilon: 0.0100\n",
      "Episode 970 - Total Reward: -336.79720069600285, Moving Average Reward: -240.16, Epsilon: 0.0100\n",
      "Episode 971 - Total Reward: -224.7386529550705, Moving Average Reward: -239.32, Epsilon: 0.0100\n",
      "Episode 972 - Total Reward: -157.65664596700302, Moving Average Reward: -239.63, Epsilon: 0.0100\n",
      "Episode 973 - Total Reward: -1665.3195928297127, Moving Average Reward: -253.66, Epsilon: 0.0100\n",
      "Episode 974 - Total Reward: -766.1788598074893, Moving Average Reward: -260.45, Epsilon: 0.0100\n",
      "Episode 975 - Total Reward: -137.8285391188731, Moving Average Reward: -260.78, Epsilon: 0.0100\n",
      "Episode 976 - Total Reward: -7413.411864818053, Moving Average Reward: -332.55, Epsilon: 0.0100\n",
      "Episode 977 - Total Reward: -229.60667039432923, Moving Average Reward: -332.89, Epsilon: 0.0100\n",
      "Episode 978 - Total Reward: -563.4770805122896, Moving Average Reward: -336.28, Epsilon: 0.0100\n",
      "Episode 979 - Total Reward: -393.7341922228321, Moving Average Reward: -336.66, Epsilon: 0.0100\n",
      "Episode 980 - Total Reward: -216.11141505439878, Moving Average Reward: -336.26, Epsilon: 0.0100\n",
      "Episode 981 - Total Reward: -196.53529513898062, Moving Average Reward: -337.17, Epsilon: 0.0100\n",
      "Episode 982 - Total Reward: -164.53406749288987, Moving Average Reward: -336.14, Epsilon: 0.0100\n",
      "Episode 983 - Total Reward: -274.9088926123196, Moving Average Reward: -336.61, Epsilon: 0.0100\n",
      "Episode 984 - Total Reward: -145.30135296270123, Moving Average Reward: -335.90, Epsilon: 0.0100\n",
      "Episode 985 - Total Reward: -658.5417362958001, Moving Average Reward: -339.32, Epsilon: 0.0100\n",
      "Episode 986 - Total Reward: -172.9522110599096, Moving Average Reward: -338.46, Epsilon: 0.0100\n",
      "Episode 987 - Total Reward: -244.56742317588575, Moving Average Reward: -337.78, Epsilon: 0.0100\n",
      "Episode 988 - Total Reward: -146.43294163291978, Moving Average Reward: -336.05, Epsilon: 0.0100\n",
      "Episode 989 - Total Reward: -137.94655781890043, Moving Average Reward: -334.51, Epsilon: 0.0100\n",
      "Episode 990 - Total Reward: -176.9085559029279, Moving Average Reward: -333.40, Epsilon: 0.0100\n",
      "Episode 991 - Total Reward: -161.75958429367836, Moving Average Reward: -333.47, Epsilon: 0.0100\n",
      "Episode 992 - Total Reward: -275.8687831991797, Moving Average Reward: -333.48, Epsilon: 0.0100\n",
      "Episode 993 - Total Reward: -297.38434642698854, Moving Average Reward: -336.05, Epsilon: 0.0100\n",
      "Episode 994 - Total Reward: -212.18809455604602, Moving Average Reward: -335.20, Epsilon: 0.0100\n",
      "Episode 995 - Total Reward: -230.1424215804518, Moving Average Reward: -336.71, Epsilon: 0.0100\n",
      "Episode 996 - Total Reward: -298.6334462676974, Moving Average Reward: -339.17, Epsilon: 0.0100\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Experience:\n",
    "    state: np.ndarray\n",
    "    action: int\n",
    "    reward: float\n",
    "    next_state: np.ndarray\n",
    "    done: bool\n",
    "    priority: float = 1.0  # Default priority\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=0.5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            capacity (int): Maximum number of experiences to store.\n",
    "            alpha (float): How much prioritization is used (0 - no prioritization, 1 - full prioritization).\n",
    "        \"\"\"\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def add(self, experience: Experience):\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size, beta=0.5, alpha=0.5):\n",
    "        if len(self.buffer) == 0:\n",
    "            return None, None, None\n",
    "\n",
    "        # Extract priorities\n",
    "        priorities = np.array([exp.priority for exp in self.buffer], dtype=np.float32)\n",
    "        scaled_priorities = priorities ** alpha\n",
    "        sample_probabilities = scaled_priorities / scaled_priorities.sum()\n",
    "\n",
    "        batch_size = min(batch_size, len(self.buffer))\n",
    "        indices = np.random.choice(len(self.buffer), size=batch_size, replace=False, p=sample_probabilities)\n",
    "        sampled_experiences = [self.buffer[idx] for idx in indices]\n",
    "\n",
    "        N = len(self.buffer)\n",
    "        weights = (1.0 / (N * sample_probabilities[indices])) ** beta\n",
    "        weights /= weights.max()  # Normalize for stability\n",
    "\n",
    "        states = np.array([exp.state for exp in sampled_experiences])\n",
    "        actions = np.array([exp.action for exp in sampled_experiences])\n",
    "        rewards = np.array([exp.reward for exp in sampled_experiences])\n",
    "        next_states = np.array([exp.next_state for exp in sampled_experiences])\n",
    "        dones = np.array([exp.done for exp in sampled_experiences])\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones), indices, weights\n",
    "\n",
    "    \n",
    "    def update_priorities(self, indices, td_errors, offset=1e-6):\n",
    "        \"\"\"\n",
    "        Update the priorities of sampled experiences.\n",
    "\n",
    "        Args:\n",
    "            indices (list or array): Indices of sampled experiences.\n",
    "            td_errors (list or array): Corresponding TD errors.\n",
    "            offset (float): Small constant to ensure no experience has zero priority.\n",
    "        \"\"\"\n",
    "        for idx, error in zip(indices, td_errors):\n",
    "            self.buffer[idx].priority = abs(error) + offset\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class DQAgent_PER:\n",
    "    def __init__(self, \n",
    "                 observation_space, \n",
    "                 action_space,\n",
    "                 gamma=0.99, \n",
    "                 lr=5e-4,\n",
    "                 buffer_size=20000,\n",
    "                 batch_size=64,\n",
    "                 epsilon_decay=0.999,\n",
    "                 epsilon=0.7,\n",
    "                 alpha=0.5,  \n",
    "                 beta=0.5,\n",
    "                 param_freq=3000,\n",
    "                 target_update_freq=1000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            observation_space (int): Dimensionality of the state space.\n",
    "            action_space (int): Number of possible actions.\n",
    "            gamma (float): Discount factor.\n",
    "            lr (float): Learning rate.\n",
    "            buffer_size (int): Replay buffer capacity.\n",
    "            batch_size (int): Training batch size.\n",
    "            epsilon_decay (float): Decay rate for epsilon.\n",
    "            epsilon (float): Initial epsilon for epsilon-greedy.\n",
    "            alpha (float): How much prioritization is used (0 - no prioritization, 1 - full prioritization).\n",
    "            beta (float): Initial value of beta for importance-sampling.\n",
    "            target_update_freq (int): Number of training steps between target network updates.\n",
    "        \"\"\"\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.alpha_decay = 0.99\n",
    "        self.beta_growth = 1.01\n",
    "        self.param_freq = param_freq\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.train_step = 0\n",
    "        \n",
    "        self.buffer = PrioritizedReplayBuffer(capacity=buffer_size, alpha=self.alpha)\n",
    "        self.model = self.build_model(name='model')\n",
    "        self.target_model = self.build_model(name='target')\n",
    "        self.update_target_network()\n",
    "    \n",
    "    def build_model(self, name):\n",
    "        model = keras.Sequential(name=name)\n",
    "        model.add(keras.Input(shape=(self.observation_space,)))\n",
    "        model.add(keras.layers.Dense(128, activation='relu'))\n",
    "        model.add(keras.layers.Dense(256, activation='relu'))\n",
    "        model.add(keras.layers.Dense(128, activation='relu'))\n",
    "        model.add(keras.layers.Dense(self.action_space, activation='linear'))\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=self.lr),\n",
    "            loss='mse'\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Store experience in replay buffer with maximum priority for new experiences.\n",
    "\n",
    "        Args:\n",
    "            state (array-like): Current state.\n",
    "            action (int): Action taken.\n",
    "            reward (float): Reward received.\n",
    "            next_state (array-like): Next state.\n",
    "            done (bool): Whether the episode ended.\n",
    "        \"\"\"\n",
    "        max_priority = max(self.buffer.buffer, key=lambda exp: exp.priority).priority if len(self.buffer) > 0 else 1.0\n",
    "        experience = Experience(state, action, reward, next_state, done, priority=max_priority)\n",
    "        self.buffer.add(experience)\n",
    "    \n",
    "    def get_param(self):\n",
    "        if self.train_step % self.param_freq == 0:\n",
    "            self.alpha *= self.alpha_decay\n",
    "            self.beta *= self.beta_growth\n",
    "        return self.alpha, self.beta\n",
    "    \n",
    "    def e_greedy(self, state):\n",
    "        \"\"\"\n",
    "        Select action using epsilon-greedy policy.\n",
    "\n",
    "        Args:\n",
    "            state (array-like): Current state.\n",
    "\n",
    "        Returns:\n",
    "            int: Selected action.\n",
    "        \"\"\"\n",
    "        if random.random() >= self.epsilon:\n",
    "            return self.predict_action(state)\n",
    "        return random.randint(0, self.action_space - 1)\n",
    "    \n",
    "    def predict_action(self, observation, epsilon=0.05):\n",
    "        \"\"\"\n",
    "        Predict the best action based on current state.\n",
    "\n",
    "        Args:\n",
    "            state (array-like): Current state.\n",
    "\n",
    "        Returns:\n",
    "            int: Action with highest predicted Q-value.\n",
    "        \"\"\"\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.random.randint(self.action_space)\n",
    "        return np.argmax(self.model.predict(np.array([observation]), verbose=False)[0])\n",
    "\n",
    "    def sample_batch(self):\n",
    "        \"\"\"\n",
    "        Sample a batch of experiences from the buffer.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (states, actions, rewards, next_states, dones), indices, IS_weights\n",
    "        \"\"\"\n",
    "        alpha, beta = self.get_param()\n",
    "        batch, indices, is_weights = self.buffer.sample(self.batch_size, beta=beta, alpha=alpha)\n",
    "        return batch, indices, is_weights\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the model using a batch of experiences from the buffer.\n",
    "        \"\"\"\n",
    "        batch, indices, is_weights = self.sample_batch()\n",
    "        if batch is None:\n",
    "            return\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = batch\n",
    "        \n",
    "        # Predict Q-values for current states\n",
    "        current_q = self.model.predict(states, verbose=0)\n",
    "        \n",
    "        # Predict Q-values for next states using target network\n",
    "        target_q = self.target_model.predict(next_states, verbose=0)\n",
    "        max_target_q = np.max(target_q, axis=1)\n",
    "        \n",
    "        # Compute target Q-values\n",
    "        for i in range(len(states)):\n",
    "            if dones[i]:\n",
    "                current_q[i][actions[i]] = rewards[i]\n",
    "            else:\n",
    "                current_q[i][actions[i]] = rewards[i] + self.gamma * max_target_q[i]\n",
    "        \n",
    "        # Compute TD errors\n",
    "        td_errors = current_q[np.arange(len(states)), actions] - self.model.predict(states, verbose=0)[np.arange(len(states)), actions]\n",
    "        \n",
    "        # Fit the model with importance-sampling weights\n",
    "        self.model.fit(states, current_q, sample_weight=is_weights, epochs=1, verbose=0)\n",
    "        \n",
    "        # Update priorities in the buffer\n",
    "        self.buffer.update_priorities(indices, td_errors)\n",
    "        \n",
    "        # Increment training step\n",
    "        self.train_step += 1\n",
    "        \n",
    "        # Update target network periodically\n",
    "        if self.train_step % self.target_update_freq == 0:\n",
    "            self.update_target_network()\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"\n",
    "        Decay the exploration rate epsilon.\n",
    "        \"\"\"\n",
    "        self.epsilon = max(0.01, self.epsilon * self.epsilon_decay)\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        self.model.save(filepath)\n",
    "    \n",
    "    def load_model(self, filepath):\n",
    "        self.model = keras.models.load_model(filepath)\n",
    "        self.update_target_network()\n",
    "\n",
    "def plot_rewards(rewards, moving_avg, window=100):\n",
    "    \"\"\"\n",
    "    Plot total rewards and moving average rewards.\n",
    "\n",
    "    Args:\n",
    "        rewards (list): Total rewards per episode.\n",
    "        moving_avg (list): Moving average rewards.\n",
    "        window (int): Window size for moving average.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(range(1, len(rewards) + 1), rewards, label='Total Reward per Episode')\n",
    "    plt.plot(range(1, len(moving_avg) + 1), moving_avg, label=f'Moving Average (Last {window}) Episodes')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title('Reward Progression Over Episodes')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Example Training Loop\n",
    "if __name__ == \"__main__\":\n",
    "    import gym\n",
    "\n",
    "    # Initialize the environment\n",
    "    env = gym.make('LunarLander-v2')\n",
    "    observation_space = env.observation_space.shape[0]  \n",
    "    action_space = env.action_space.n  \n",
    "\n",
    "    # Initialize the PER DQN agent\n",
    "    agent = DQAgent_PER(\n",
    "        observation_space=observation_space,\n",
    "        action_space=action_space,\n",
    "        gamma=0.99,\n",
    "        lr=5e-4,\n",
    "        buffer_size=20000,\n",
    "        batch_size=64,\n",
    "        epsilon_decay=0.995,\n",
    "        epsilon=1.0,  # Start with full exploration\n",
    "        alpha=0.5,\n",
    "        beta=0.5,\n",
    "        param_freq=1000,\n",
    "        target_update_freq=1000\n",
    "    )\n",
    "\n",
    "    num_episodes = 1000\n",
    "    rewards_per_episode = []\n",
    "    moving_average_window = 100\n",
    "    moving_averages = []\n",
    "    recent_rewards = deque(maxlen=moving_average_window)\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        step_count = 0\n",
    "\n",
    "        while not done:\n",
    "            action = agent.e_greedy(state)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            step_count += 1\n",
    "            agent.train()\n",
    "\n",
    "        # Decay epsilon after each episode\n",
    "        agent.decay_epsilon()\n",
    "\n",
    "        rewards_per_episode.append(total_reward)\n",
    "        recent_rewards.append(total_reward)\n",
    "        moving_avg = np.mean(recent_rewards)\n",
    "        moving_averages.append(moving_avg)\n",
    "\n",
    "        print(f\"Episode {episode + 1} - Total Reward: {total_reward}, Moving Average Reward: {moving_avg:.2f}, Epsilon: {agent.epsilon:.4f}\")\n",
    "\n",
    "        # Plotting periodically\n",
    "        # if (episode + 1) % 100 == 0:\n",
    "        #     plot_rewards(rewards_per_episode, moving_averages, window=moving_average_window)\n",
    "\n",
    "        if moving_avg >= 200 and episode >= moving_average_window:\n",
    "            print(f\"\\nEnvironment solved in {episode + 1} episodes with moving average reward {moving_avg:.2f}!\")\n",
    "            agent.save_model(\"dqn_cartpole_per.h5\")\n",
    "            break\n",
    "\n",
    "    # Final Plot\n",
    "    plot_rewards(rewards_per_episode, moving_averages, window=moving_average_window)\n",
    "    agent.save_model(\"dqn_cartpole_per_final.h5\")\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
